{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "keras+binary+classfication_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Bd8SswENObD7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('myenv': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "interpreter": {
      "hash": "c205a936699ab56433d61bd6e0e1685d6e8acb20d207dedd395bc61ee66c6559"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Input libraries"
      ],
      "metadata": {
        "id": "H826CHWXObDj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import keras\r\n",
        "import numpy\r\n",
        "import pandas\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "metadata": {
        "id": "jhKb3xzpObDn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "import torch\r\n",
        "torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "metadata": {
        "id": "sT2nt1wDObDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data input"
      ],
      "metadata": {
        "id": "BBK3_GXtObDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Importing the dataset\r\n",
        "dataset = pd.read_csv('default of credit card clients.csv')\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "QBT8ES5MObDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "8BZCS8EMOv4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "import pandas as pd\r\n",
        "from sklearn import preprocessing"
      ],
      "outputs": [],
      "metadata": {
        "id": "IHcH4NV1ObDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X and Y distribution of dataset"
      ],
      "metadata": {
        "id": "dUgQEfi8ObDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "X = dataset.iloc[:, 1:24].values\r\n",
        "y = dataset.iloc[:, 24].values\r\n",
        "X.shape,y.shape\r\n",
        "type(y)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q4UbP-AObDp",
        "outputId": "f3dd4e36-e884-4a64-bcff-4d193ee87d67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "X.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX79O2KiObDr",
        "outputId": "82aa8c8a-fd65-4d7d-8855-a562fe16a2bf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "import pandas as pd\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\r\n",
        "X = min_max_scaler.fit_transform(X)\r\n",
        "df = pd.DataFrame(X)\r\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0    1         2         3         4    5    6    7    8    9   \\\n",
              "0      0.010101  1.0  0.333333  0.333333  0.051724  0.4  0.4  0.1  0.1  0.0   \n",
              "1      0.111111  1.0  0.333333  0.666667  0.086207  0.1  0.4  0.2  0.2  0.2   \n",
              "2      0.080808  1.0  0.333333  0.666667  0.224138  0.2  0.2  0.2  0.2  0.2   \n",
              "3      0.040404  1.0  0.333333  0.333333  0.275862  0.2  0.2  0.2  0.2  0.2   \n",
              "4      0.040404  0.0  0.333333  0.333333  0.620690  0.1  0.2  0.1  0.2  0.2   \n",
              "...         ...  ...       ...       ...       ...  ...  ...  ...  ...  ...   \n",
              "29995  0.212121  0.0  0.500000  0.333333  0.310345  0.2  0.2  0.2  0.2  0.2   \n",
              "29996  0.141414  0.0  0.500000  0.666667  0.379310  0.1  0.1  0.1  0.1  0.2   \n",
              "29997  0.020202  0.0  0.333333  0.666667  0.275862  0.6  0.5  0.4  0.1  0.2   \n",
              "29998  0.070707  0.0  0.500000  0.333333  0.344828  0.3  0.1  0.2  0.2  0.2   \n",
              "29999  0.040404  0.0  0.333333  0.333333  0.431034  0.2  0.2  0.2  0.2  0.2   \n",
              "\n",
              "       ...        13        14        15        16        17        18  \\\n",
              "0      ...  0.086723  0.160138  0.080648  0.260979  0.000000  0.000409   \n",
              "1      ...  0.087817  0.163220  0.084074  0.263485  0.000000  0.000594   \n",
              "2      ...  0.093789  0.173637  0.095470  0.272928  0.001738  0.000891   \n",
              "3      ...  0.113407  0.186809  0.109363  0.283685  0.002290  0.001199   \n",
              "4      ...  0.106020  0.179863  0.099633  0.275681  0.002290  0.021779   \n",
              "...    ...       ...       ...       ...       ...       ...       ...   \n",
              "29995  ...  0.200746  0.243036  0.111622  0.273259  0.009730  0.011875   \n",
              "29996  ...  0.088267  0.168596  0.085794  0.260979  0.002103  0.002094   \n",
              "29997  ...  0.087859  0.179805  0.101057  0.275854  0.000000  0.000000   \n",
              "29998  ...  0.128239  0.209850  0.092403  0.298591  0.098334  0.002024   \n",
              "29999  ...  0.113667  0.194553  0.112803  0.272746  0.002379  0.001069   \n",
              "\n",
              "             19        20        21        22  \n",
              "0      0.000000  0.000000  0.000000  0.000000  \n",
              "1      0.001116  0.001610  0.000000  0.003783  \n",
              "2      0.001116  0.001610  0.002345  0.009458  \n",
              "3      0.001339  0.001771  0.002506  0.001892  \n",
              "4      0.011160  0.014493  0.001615  0.001284  \n",
              "...         ...       ...       ...       ...  \n",
              "29995  0.005583  0.004907  0.011723  0.001892  \n",
              "29996  0.010042  0.000208  0.000000  0.000000  \n",
              "29997  0.024552  0.006763  0.004689  0.005864  \n",
              "29998  0.001315  0.003101  0.124174  0.003412  \n",
              "29999  0.001596  0.001610  0.002345  0.001892  \n",
              "\n",
              "[30000 rows x 23 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.051724</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.086723</td>\n",
              "      <td>0.160138</td>\n",
              "      <td>0.080648</td>\n",
              "      <td>0.260979</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.111111</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.086207</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.087817</td>\n",
              "      <td>0.163220</td>\n",
              "      <td>0.084074</td>\n",
              "      <td>0.263485</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.080808</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.224138</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.093789</td>\n",
              "      <td>0.173637</td>\n",
              "      <td>0.095470</td>\n",
              "      <td>0.272928</td>\n",
              "      <td>0.001738</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.002345</td>\n",
              "      <td>0.009458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.040404</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.113407</td>\n",
              "      <td>0.186809</td>\n",
              "      <td>0.109363</td>\n",
              "      <td>0.283685</td>\n",
              "      <td>0.002290</td>\n",
              "      <td>0.001199</td>\n",
              "      <td>0.001339</td>\n",
              "      <td>0.001771</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>0.001892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.106020</td>\n",
              "      <td>0.179863</td>\n",
              "      <td>0.099633</td>\n",
              "      <td>0.275681</td>\n",
              "      <td>0.002290</td>\n",
              "      <td>0.021779</td>\n",
              "      <td>0.011160</td>\n",
              "      <td>0.014493</td>\n",
              "      <td>0.001615</td>\n",
              "      <td>0.001284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29995</th>\n",
              "      <td>0.212121</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.310345</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200746</td>\n",
              "      <td>0.243036</td>\n",
              "      <td>0.111622</td>\n",
              "      <td>0.273259</td>\n",
              "      <td>0.009730</td>\n",
              "      <td>0.011875</td>\n",
              "      <td>0.005583</td>\n",
              "      <td>0.004907</td>\n",
              "      <td>0.011723</td>\n",
              "      <td>0.001892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29996</th>\n",
              "      <td>0.141414</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088267</td>\n",
              "      <td>0.168596</td>\n",
              "      <td>0.085794</td>\n",
              "      <td>0.260979</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.002094</td>\n",
              "      <td>0.010042</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29997</th>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.275862</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.087859</td>\n",
              "      <td>0.179805</td>\n",
              "      <td>0.101057</td>\n",
              "      <td>0.275854</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.024552</td>\n",
              "      <td>0.006763</td>\n",
              "      <td>0.004689</td>\n",
              "      <td>0.005864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29998</th>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.344828</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128239</td>\n",
              "      <td>0.209850</td>\n",
              "      <td>0.092403</td>\n",
              "      <td>0.298591</td>\n",
              "      <td>0.098334</td>\n",
              "      <td>0.002024</td>\n",
              "      <td>0.001315</td>\n",
              "      <td>0.003101</td>\n",
              "      <td>0.124174</td>\n",
              "      <td>0.003412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29999</th>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.113667</td>\n",
              "      <td>0.194553</td>\n",
              "      <td>0.112803</td>\n",
              "      <td>0.272746</td>\n",
              "      <td>0.002379</td>\n",
              "      <td>0.001069</td>\n",
              "      <td>0.001596</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.002345</td>\n",
              "      <td>0.001892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30000 rows Ã— 23 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "EIMn8nogObDr",
        "outputId": "0ae22be9-89f0-41ae-88bf-bbe3e842f10a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "X[1]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.11111111e-01, 1.00000000e+00, 3.33333333e-01, 6.66666667e-01,\n",
              "       8.62068966e-02, 1.00000000e-01, 4.00000000e-01, 2.00000000e-01,\n",
              "       2.00000000e-01, 2.00000000e-01, 4.00000000e-01, 1.48892434e-01,\n",
              "       6.78575089e-02, 8.78171337e-02, 1.63219937e-01, 8.40739510e-02,\n",
              "       2.63484742e-01, 0.00000000e+00, 5.93732912e-04, 1.11602161e-03,\n",
              "       1.61030596e-03, 0.00000000e+00, 3.78310691e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-muG2YTObDs",
        "outputId": "5f1a45af-234f-454c-8295-9a0cb99f2a6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "rpJKepTkObDs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "X[1]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.11111111e-01, 1.00000000e+00, 3.33333333e-01, 6.66666667e-01,\n",
              "       8.62068966e-02, 1.00000000e-01, 4.00000000e-01, 2.00000000e-01,\n",
              "       2.00000000e-01, 2.00000000e-01, 4.00000000e-01, 1.48892434e-01,\n",
              "       6.78575089e-02, 8.78171337e-02, 1.63219937e-01, 8.40739510e-02,\n",
              "       2.63484742e-01, 0.00000000e+00, 5.93732912e-04, 1.11602161e-03,\n",
              "       1.61030596e-03, 0.00000000e+00, 3.78310691e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy8oKwrvObDs",
        "outputId": "c942c280-6034-4a20-fca5-e787359b4a9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Categorical to Numerical Data"
      ],
      "metadata": {
        "id": "u2urUFy9ObDs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\r\n",
        "# #for country column\r\n",
        "# labelencoder_X_1 = LabelEncoder()\r\n",
        "# X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\r\n",
        "# #for gender column\r\n",
        "# labelencoder_X_2 = LabelEncoder()\r\n",
        "# X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\r\n",
        "# np.shape(X)\r\n",
        "# X"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q7-rzBeEObDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Baseline Model-Using Buildin Function"
      ],
      "metadata": {
        "id": "g1cBuvrnObDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# # baseline model(using 1 connected layer and one 1 output layer)\r\n",
        "# def create_baseline():\r\n",
        "# # create model\r\n",
        "#     model = Sequential()\r\n",
        "#     model.add(Dense(30, input_dim=23, kernel_initializer='normal', activation='relu'))\r\n",
        "#     model.add(Dense(23, input_dim=30, kernel_initializer='normal', activation='relu'))\r\n",
        "#     model.add(Dense(30, input_dim=23, kernel_initializer='normal', activation='relu'))\r\n",
        "#     model.add(Dense(30, input_dim=23, kernel_initializer='normal', activation='relu'))\r\n",
        "#     model.add(Dense(1, kernel_initializer='random_normal', activation='tanh'))\r\n",
        "#     # Compile model\r\n",
        "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "#     return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "glQf7tE5ObDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# seed = 7\r\n",
        "# numpy.random.seed(seed)\r\n",
        "# # evaluate model with standardized dataset\r\n",
        "# estimator = KerasClassifier(build_fn=create_baseline, epochs=50, batch_size=5, verbose=0)\r\n",
        "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\r\n",
        "# results = cross_val_score(estimator, X, y, cv=kfold)\r\n",
        "# print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "pa_w9j9IObDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Model-Using Sequential Module\n"
      ],
      "metadata": {
        "id": "C3B2vxOWObDu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\r\n",
        "X_train"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.03030303e-01, 0.00000000e+00, 1.66666667e-01, ...,\n",
              "        2.76167472e-03, 2.79840292e-02, 5.23960308e-04],\n",
              "       [2.02020202e-01, 1.00000000e+00, 1.66666667e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [2.02020202e-02, 0.00000000e+00, 3.33333333e-01, ...,\n",
              "        0.00000000e+00, 3.51675970e-03, 3.78310691e-03],\n",
              "       ...,\n",
              "       [1.91919192e-01, 1.00000000e+00, 3.33333333e-01, ...,\n",
              "        2.41545894e-03, 3.51675970e-03, 2.83733019e-03],\n",
              "       [3.03030303e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
              "        1.71014493e-03, 3.75121035e-03, 2.91677543e-03],\n",
              "       [1.51515152e-01, 1.00000000e+00, 8.33333333e-01, ...,\n",
              "        7.19806763e-03, 1.03627186e-02, 1.20113645e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if4vA-AWObDu",
        "outputId": "f25ca478-60b2-406a-e851-522c182b0644"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "type(X_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQYvbgnwObDu",
        "outputId": "f4f9d310-4246-49aa-9689-761c51dac2be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "X_train"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.03030303e-01, 0.00000000e+00, 1.66666667e-01, ...,\n",
              "        2.76167472e-03, 2.79840292e-02, 5.23960308e-04],\n",
              "       [2.02020202e-01, 1.00000000e+00, 1.66666667e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [2.02020202e-02, 0.00000000e+00, 3.33333333e-01, ...,\n",
              "        0.00000000e+00, 3.51675970e-03, 3.78310691e-03],\n",
              "       ...,\n",
              "       [1.91919192e-01, 1.00000000e+00, 3.33333333e-01, ...,\n",
              "        2.41545894e-03, 3.51675970e-03, 2.83733019e-03],\n",
              "       [3.03030303e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
              "        1.71014493e-03, 3.75121035e-03, 2.91677543e-03],\n",
              "       [1.51515152e-01, 1.00000000e+00, 8.33333333e-01, ...,\n",
              "        7.19806763e-03, 1.03627186e-02, 1.20113645e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBGXvdy3ObDv",
        "outputId": "c9c03220-e0d0-4ea0-90ec-176b7a7465b3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "y.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000,)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzYWSkoKObDv",
        "outputId": "3a4e95f3-a9f6-4f13-bf73-8b092d313aa0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "import tensorflow as tf\r\n",
        "# import keras\r\n",
        "# from keras import layers\r\n",
        "# from keras import models\r\n",
        "# from keras import utils\r\n",
        "# from keras.layers import Dense\r\n",
        "# from keras.models import Sequential\r\n",
        "# from keras.layers import Flatten\r\n",
        "# from keras.layers import Dropout\r\n",
        "# from keras.layers import Activation\r\n",
        "# from keras.regularizers import l2\r\n",
        "# from keras.optimizers import SGD\r\n",
        "# from keras.optimizers import RMSprop\r\n",
        "# from keras import datasets\r\n",
        "\r\n",
        "# from keras.callbacks import LearningRateScheduler\r\n",
        "# from keras.callbacks import History\r\n",
        "\r\n",
        "# from keras import losses\r\n",
        "# from sklearn.utils import shuffle\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.optimizers import SGD\r\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "print(tf.__version__)\r\n",
        "print(tf.keras.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.0\n",
            "2.5.0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OqG-5K0ObDv",
        "outputId": "64780460-0e45-4ed5-dab9-a6d9d6ad5f55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\r\n",
        "    initial_learning_rate=1e-2,\r\n",
        "    decay_steps=10000,\r\n",
        "    decay_rate=0.1)\r\n",
        "optimizer1 = keras.optimizers.SGD(learning_rate=lr_schedule)\r\n",
        "optimizer2 = keras.optimizers.Adam(learning_rate = 0.001,beta_1=0.9,\r\n",
        "    beta_2=0.999,\r\n",
        "    epsilon=1e-07)"
      ],
      "outputs": [],
      "metadata": {
        "id": "umggJB7jObDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "my_callbacks = [\r\n",
        "    tf.keras.callbacks.EarlyStopping(patience=2),\r\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{loss:.2f}.h5'),\r\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n",
        "]"
      ],
      "outputs": [],
      "metadata": {
        "id": "rNmdczm0ObDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "# #Initializing Neural Network\r\n",
        "# Model = keras.Sequential()\r\n",
        "# Model.add(Dense(100, input_dim = 23, kernel_initializer = 'random_uniform', activation = 'relu'))\r\n",
        "# # Adding the second hidden layer\r\n",
        "# Model.add(Dense(200, kernel_initializer = 'random_uniform', activation = 'relu'))\r\n",
        "# # Adding the third hidden layer\r\n",
        "# Model.add(Dense(300, kernel_initializer = 'uniform', activation = 'relu'))\r\n",
        "# # Adding the Fourth hidden layer\r\n",
        "# Model.add(Dense(200, kernel_initializer = 'random_uniform', activation = 'relu'))\r\n",
        "# # Adding the Fifth layer\r\n",
        "# Model.add(Dense(200, kernel_initializer = 'random_uniform', activation = 'relu'))\r\n",
        "# # Adding the output layer\r\n",
        "# Model.add(Dense(1, kernel_initializer = 'random_uniform', activation = 'sigmoid'))\r\n",
        "# # Compiling Neural Network\r\n",
        "# Model.compile(optimizer = optimizer2, loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n",
        "# #fitting the neural Network\r\n",
        "# early_stopping_monitor = EarlyStopping(patience=2)\r\n",
        "# Model.fit(X_train, y_train, batch_size = 15, epochs = 50)"
      ],
      "outputs": [],
      "metadata": {
        "id": "tdzxJQSsObDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "# score = Model.evaluate(X_test, y_test, verbose=1)\r\n",
        "# print('Accuracy: ', score[1]*100)\r\n",
        "# print( 'loss:', score[0]*100)"
      ],
      "outputs": [],
      "metadata": {
        "id": "I0Cx7Yk-ObDy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "dqTma-vOObDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "# print(optimizer2.get_weights())"
      ],
      "outputs": [],
      "metadata": {
        "id": "oMeOyKhpObDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Implementation\n",
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "aRS3HVi4ObD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "import torch\r\n",
        "# import keras\r\n",
        "import numpy\r\n",
        "import pandas\r\n",
        "# from keras.models import Sequential\r\n",
        "# from keras.layers import Dense\r\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "metadata": {
        "id": "tnwZzp66ObD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Start"
      ],
      "metadata": {
        "id": "TM56r96DObD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "X_train"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.03030303e-01, 0.00000000e+00, 1.66666667e-01, ...,\n",
              "        2.76167472e-03, 2.79840292e-02, 5.23960308e-04],\n",
              "       [2.02020202e-01, 1.00000000e+00, 1.66666667e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [2.02020202e-02, 0.00000000e+00, 3.33333333e-01, ...,\n",
              "        0.00000000e+00, 3.51675970e-03, 3.78310691e-03],\n",
              "       ...,\n",
              "       [1.91919192e-01, 1.00000000e+00, 3.33333333e-01, ...,\n",
              "        2.41545894e-03, 3.51675970e-03, 2.83733019e-03],\n",
              "       [3.03030303e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
              "        1.71014493e-03, 3.75121035e-03, 2.91677543e-03],\n",
              "       [1.51515152e-01, 1.00000000e+00, 8.33333333e-01, ...,\n",
              "        7.19806763e-03, 1.03627186e-02, 1.20113645e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VMz4ikLObD0",
        "outputId": "9a6f50aa-67d6-4cae-9709-0de067839ae5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "y_train"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqW5ODZuObD1",
        "outputId": "a1e48878-11b1-4ad4-e578-ab3a393ba00a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "X_test"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07070707, 1.        , 0.16666667, ..., 0.        , 0.        ,\n",
              "        0.00162674],\n",
              "       [0.19191919, 1.        , 0.16666667, ..., 0.00165539, 0.00205848,\n",
              "        0.00166078],\n",
              "       [0.09090909, 1.        , 0.33333333, ..., 0.00644122, 0.01172253,\n",
              "        0.01513243],\n",
              "       ...,\n",
              "       [0.11111111, 1.        , 0.33333333, ..., 0.00660225, 0.00586127,\n",
              "        0.        ],\n",
              "       [0.09090909, 1.        , 0.33333333, ..., 0.00515298, 0.        ,\n",
              "        0.00378311],\n",
              "       [0.33333333, 0.        , 0.33333333, ..., 0.01869565, 0.01172253,\n",
              "        0.00561981]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpU6tGArObD1",
        "outputId": "b304f919-227d-4b97-bdfa-ec538cf00fbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "y_test"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 1], dtype=int64)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA0tPHxNObD1",
        "outputId": "c843f280-f0ee-4ca7-c10e-d17ac810ee97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "data = [[1, 2],[3, 4]]\r\n",
        "x_data = torch.tensor(data)"
      ],
      "outputs": [],
      "metadata": {
        "id": "O80NWTcyObD1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "x_data"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2rju_sJObD2",
        "outputId": "f9cda5e9-f873-49fd-8469-a65d66b77148"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to Tensors"
      ],
      "metadata": {
        "id": "lzFrQl4JObD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "# X_train = torch.tensor(X_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0DNo2zlXObD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "X_train.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJNnFbFaObD2",
        "outputId": "89cfe0ae-e9ac-4f92-b534-ab8973b869d3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "y_train"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Pn5t91ObD3",
        "outputId": "e24636d9-4283-4cbd-f117-1602d2fda8b3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "y_test"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 1], dtype=int64)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuxhOprmObD3",
        "outputId": "71f25c49-eaee-4cd1-950f-e632e5e4bfe8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torch.nn.functional as F"
      ],
      "outputs": [],
      "metadata": {
        "id": "OHPP_Cu5ObD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Empty Cache"
      ],
      "metadata": {
        "id": "AED5tTYlObD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "uAwmeCCVObD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "y_train_tens=y_train.astype(float)\r\n",
        "X_test_tens = X_test.astype(float)\r\n",
        "y_test_tens = y_test.astype(float)\r\n",
        "X_train_tens=X_train.astype(float)\r\n",
        "y_train.shape[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24000"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIZEew4EObD4",
        "outputId": "84a4ae8d-9d04-4360-e629-0821b9ad161d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "# import torch\r\n",
        "\r\n",
        "# # X=X_iris_tr; Y=Y_iris_tr; X_val=X_iris_val; Y_val=Y_iris_val\r\n",
        "# # del X, Y, X_val, Y_val\r\n",
        "\r\n",
        "# def two_layer_regression_autograd_train(X, Y, X_val, Y_val, lr, nite):\r\n",
        "\r\n",
        "#     dtype = torch.float\r\n",
        "#     device = torch.device(\"cpu\")\r\n",
        "#     # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\r\n",
        "\r\n",
        "#     # N is batch size; D_in is input dimension;\r\n",
        "#     # H is hidden dimension; D_out is output dimension.\r\n",
        "#     N, D_in, H, D_out = X.shape[0], X.shape[1], 200, Y.shape[0]\r\n",
        "\r\n",
        "#     # Setting requires_grad=False indicates that we do not need to compute gradients\r\n",
        "#     # with respect to these Tensors during the backward pass.\r\n",
        "#     X = torch.from_numpy(X).float()\r\n",
        "#     Y = torch.from_numpy(Y).float()\r\n",
        "#     X_val = torch.from_numpy(X_val).float()\r\n",
        "#     Y_val = torch.from_numpy(Y_val).float()\r\n",
        "\r\n",
        "#     # Create random Tensors for weights.\r\n",
        "#     # Setting requires_grad=True indicates that we want to compute gradients with\r\n",
        "#     # respect to these Tensors during the backward pass.\r\n",
        "#     W1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\r\n",
        "#     W2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\r\n",
        "\r\n",
        "#     losses_tr, losses_val = list(), list()\r\n",
        "\r\n",
        "#     learning_rate = lr\r\n",
        "#     for t in range(nite):\r\n",
        "#         # Forward pass: compute predicted y using operations on Tensors; these\r\n",
        "#         # are exactly the same operations we used to compute the forward pass using\r\n",
        "#         # Tensors, but we do not need to keep references to intermediate values since\r\n",
        "#         # we are not implementing the backward pass by hand.\r\n",
        "#         y_pred = X.mm(W1).clamp(min=0).mm(W2)\r\n",
        "\r\n",
        "#         # Compute and print loss using operations on Tensors.\r\n",
        "#         # Now loss is a Tensor of shape (1,)\r\n",
        "#         # loss.item() gets the scalar value held in the loss.\r\n",
        "#         loss = (y_pred - Y).pow(2).sum()\r\n",
        "\r\n",
        "#         # Use autograd to compute the backward pass. This call will compute the\r\n",
        "#         # gradient of loss with respect to all Tensors with requires_grad=True.\r\n",
        "#         # After this call w1.grad and w2.grad will be Tensors holding the gradient\r\n",
        "#         # of the loss with respect to w1 and w2 respectively.\r\n",
        "#         loss.backward()\r\n",
        "\r\n",
        "#         # Manually update weights using gradient descent. Wrap in torch.no_grad()\r\n",
        "#         # because weights have requires_grad=True, but we don't need to track this\r\n",
        "#         # in autograd.\r\n",
        "#         # An alternative way is to operate on weight.data and weight.grad.data.\r\n",
        "#         # Recall that tensor.data gives a tensor that shares the storage with\r\n",
        "#         # tensor, but doesn't track history.\r\n",
        "#         # You can also use torch.optim.SGD to achieve this.\r\n",
        "#         with torch.no_grad():\r\n",
        "#             W1 -= learning_rate * W1.grad\r\n",
        "#             W2 -= learning_rate * W2.grad\r\n",
        "\r\n",
        "#             # Manually zero the gradients after updating weights\r\n",
        "#             W1.grad.zero_()\r\n",
        "#             W2.grad.zero_()\r\n",
        "\r\n",
        "#             y_pred = X_val.mm(W1).clamp(min=0).mm(W2)\r\n",
        "\r\n",
        "#             # Compute and print loss using operations on Tensors.\r\n",
        "#             # Now loss is a Tensor of shape (1,)\r\n",
        "#             # loss.item() gets the scalar value held in the loss.\r\n",
        "#             loss_val = (y_pred - Y).pow(2).sum()\r\n",
        "\r\n",
        "#         if t % 10 == 0:\r\n",
        "#             print(t, loss.item(), loss_val.item())\r\n",
        "\r\n",
        "#         losses_tr.append(loss.item())\r\n",
        "#         losses_val.append(loss_val.item())\r\n",
        "\r\n",
        "#     return W1, W2, losses_tr, losses_val\r\n",
        "\r\n",
        "# W1, W2, losses_tr, losses_val = two_layer_regression_autograd_train(X=X_train, Y=y_train, X_val=X_test, Y_val=y_test,\r\n",
        "#                                                                  lr=1e-4, nite=50)\r\n",
        "# plt.plot(np.arange(len(losses_tr)), losses_tr, \"-b\", np.arange(len(losses_val)), losses_val, \"-r\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "UsgSKqKLObD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "y_train =torch.tensor(y_train_tens,requires_grad = True) \r\n",
        "X_test  =torch.tensor(X_test_tens  ,requires_grad = True)\r\n",
        "y_test  =torch.tensor(y_test_tens  ,requires_grad = True)\r\n",
        "X_train =torch.tensor(X_train_tens,requires_grad = True) \r\n",
        "print(X_train, y_train.shape[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.0303e-01, 0.0000e+00, 1.6667e-01,  ..., 2.7617e-03, 2.7984e-02,\n",
            "         5.2396e-04],\n",
            "        [2.0202e-01, 1.0000e+00, 1.6667e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [2.0202e-02, 0.0000e+00, 3.3333e-01,  ..., 0.0000e+00, 3.5168e-03,\n",
            "         3.7831e-03],\n",
            "        ...,\n",
            "        [1.9192e-01, 1.0000e+00, 3.3333e-01,  ..., 2.4155e-03, 3.5168e-03,\n",
            "         2.8373e-03],\n",
            "        [3.0303e-02, 1.0000e+00, 3.3333e-01,  ..., 1.7101e-03, 3.7512e-03,\n",
            "         2.9168e-03],\n",
            "        [1.5152e-01, 1.0000e+00, 8.3333e-01,  ..., 7.1981e-03, 1.0363e-02,\n",
            "         1.2011e-02]], dtype=torch.float64, requires_grad=True) 24000\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_TZCcVSObD5",
        "outputId": "32d622fc-e3b6-4b13-ad44-c596cb413575"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "bhLbtTcISvc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Sequential Model and Training with SGD Optimizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "model = nn.Sequential(nn.Linear(23, 256),\r\n",
        "                      nn.ReLU(),\r\n",
        "                      nn.Linear(256, 64),\r\n",
        "                      nn.ReLU(),\r\n",
        "                      nn.Linear(64, 1),\r\n",
        "                      nn.LogSoftmax(dim=1))\r\n",
        "# Define the loss\r\n",
        "criterion = nn.BCELoss()\r\n",
        "# Optimizers require the parameters to optimize and a learning rate\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n",
        "epochs = 5\r\n",
        "limit = 100\r\n",
        "for e in range(epochs):\r\n",
        "    running_loss = 0\r\n",
        "    for i, x in zip(range(e*limit,(e+1)*limit), X_train):\r\n",
        "        x = x.float()\r\n",
        "        x = x.view(-1,x.shape[0])\r\n",
        "        # Training pass\r\n",
        "        optimizer.zero_grad()\r\n",
        "        y = y_train.view(y_train.shape[0],1)\r\n",
        "        # y = y.type(torch.LongTensor)\r\n",
        "        label = y[i].view(1,1)\r\n",
        "        # label = torch.full((1,1), y[i], dtype=torch.float)\r\n",
        "        label = label.float()\r\n",
        "        output = model(x)\r\n",
        "        # print(output.shape,label.shape)\r\n",
        "        loss = criterion(output, label.detach())\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        # print(loss,output, label )\r\n",
        "        running_loss += loss.item()\r\n",
        "    else:\r\n",
        "        pass\r\n",
        "        # print(f\"Training loss: {running_loss/limit}\")\r\n",
        "    print(f\"Epochs: {e},running_loss: {running_loss/100},accuracy: {((limit-(running_loss/100))/limit)*100}\")\r\n",
        "    for name, param in model.named_parameters():\r\n",
        "        print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\r\n",
        "# PATH = \"C:\\Users\\dewth\\Documents\\Credible-Not-Credible-Model\"\r\n",
        "# torch.save(model.state_dict(), PATH)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 0,running_loss: 21.0,accuracy: 79.0\n",
            "Layer: 0.weight | Size: torch.Size([256, 23]) | Values : tensor([[ 0.0334,  0.1095,  0.0535, -0.1839,  0.1524,  0.1039,  0.1624,  0.0865,\n",
            "          0.0447, -0.1215, -0.0344, -0.0420, -0.0399,  0.1605,  0.1428, -0.1647,\n",
            "          0.0978,  0.0232, -0.1973, -0.0765,  0.0285, -0.1268,  0.0600],\n",
            "        [ 0.0876, -0.0514, -0.1535,  0.0975, -0.1003, -0.0278, -0.1122,  0.1575,\n",
            "          0.1764,  0.0404,  0.1649, -0.1364, -0.1664,  0.0460,  0.0958,  0.0899,\n",
            "         -0.1433,  0.0765,  0.0801, -0.0380,  0.1858,  0.0374, -0.1264]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 0.bias | Size: torch.Size([256]) | Values : tensor([-0.0540,  0.0107], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 2.weight | Size: torch.Size([64, 256]) | Values : tensor([[-5.8623e-02,  3.1298e-02,  4.8088e-02,  2.0751e-02,  4.9323e-03,\n",
            "         -4.9633e-02, -4.1430e-02, -6.0132e-02, -1.2477e-02,  3.0073e-03,\n",
            "          1.6044e-02,  2.9084e-02,  4.6019e-02, -2.6304e-02, -8.3184e-03,\n",
            "          3.1041e-02, -1.8753e-02, -3.8682e-02,  5.8958e-02, -7.9693e-03,\n",
            "         -4.1230e-02, -3.6553e-02, -4.3637e-02,  6.0021e-02, -3.2571e-02,\n",
            "         -6.8401e-03,  9.3616e-03, -3.8027e-02,  5.4041e-03, -1.8520e-02,\n",
            "         -3.5970e-02, -3.4900e-02,  4.5161e-03,  5.4626e-02,  4.1976e-02,\n",
            "         -1.2721e-03,  2.5226e-02,  6.1520e-02,  1.8663e-02, -4.6688e-02,\n",
            "          2.7044e-02,  2.3453e-02, -2.8371e-02,  1.7704e-02,  1.8997e-02,\n",
            "         -2.0838e-02, -2.5912e-02, -4.0489e-02, -3.0490e-02,  1.6585e-02,\n",
            "          7.5735e-03,  5.2090e-02, -3.0198e-02, -4.9985e-02, -5.5186e-02,\n",
            "         -2.2005e-02,  5.8541e-02,  4.9938e-02,  1.6151e-02,  2.7960e-02,\n",
            "         -3.5706e-02,  3.6234e-02, -1.9187e-02,  2.3055e-02, -5.5042e-02,\n",
            "         -2.0912e-02, -4.8102e-02, -3.9940e-02, -1.9925e-02,  3.9085e-02,\n",
            "         -1.1768e-03, -4.3147e-03, -1.0583e-02, -2.5338e-02, -4.3769e-02,\n",
            "          5.0673e-02,  3.8777e-02, -3.8908e-02, -1.9873e-02, -1.0981e-02,\n",
            "         -4.0490e-02,  3.8214e-02,  4.1300e-02,  3.2931e-02, -2.8845e-02,\n",
            "          4.7091e-02,  3.1983e-02,  4.7104e-02, -2.3783e-02, -5.8941e-02,\n",
            "          5.9361e-02,  5.8680e-02,  5.4543e-03, -1.7428e-02,  1.2293e-02,\n",
            "          1.6191e-02,  4.7547e-02, -1.7308e-02, -2.7145e-02,  2.3784e-02,\n",
            "          4.3072e-02,  3.0716e-03,  4.4007e-02,  1.3740e-03,  6.1135e-03,\n",
            "         -5.5594e-02, -2.6177e-02,  2.7192e-03, -4.3180e-02,  8.4083e-03,\n",
            "         -4.3120e-02, -8.9889e-03, -3.3969e-03, -5.1247e-02,  1.3959e-02,\n",
            "         -3.9742e-02, -3.3993e-02,  8.1451e-03, -1.2535e-02,  4.8600e-02,\n",
            "         -3.1399e-02, -1.5234e-02,  5.2034e-02, -1.9631e-02, -2.9294e-03,\n",
            "          7.7736e-03,  2.8451e-02, -5.8729e-02,  2.3520e-03,  5.7636e-02,\n",
            "          2.4307e-02,  6.0702e-02, -6.2082e-02,  5.2472e-02,  4.9470e-03,\n",
            "          3.2662e-02,  4.2935e-02,  3.5921e-02,  4.3097e-02, -3.2475e-02,\n",
            "         -4.4825e-03,  1.4024e-02, -2.2177e-02,  1.4418e-02, -2.1048e-02,\n",
            "          2.0322e-02, -9.5908e-03,  4.1660e-03, -4.2933e-02,  5.8732e-02,\n",
            "         -1.0700e-02, -1.7704e-03,  3.8965e-02,  1.4623e-02,  7.0755e-03,\n",
            "          3.8006e-02, -4.0189e-02, -6.0178e-02,  3.5217e-02,  2.6748e-02,\n",
            "         -2.0527e-02, -1.1958e-03, -5.4087e-02, -3.5670e-02,  4.1478e-02,\n",
            "          1.7810e-03,  5.2662e-02,  5.0116e-02,  4.6311e-02, -1.0135e-03,\n",
            "         -3.4414e-02,  5.4454e-02,  1.9309e-02,  1.3896e-02, -1.4773e-02,\n",
            "          6.1425e-04, -2.1494e-03,  2.4583e-02,  5.1489e-02, -3.6403e-02,\n",
            "         -3.5545e-02,  4.3674e-02,  1.4799e-02, -2.7804e-02,  2.5681e-02,\n",
            "         -2.0811e-02, -5.5847e-02, -7.7742e-04,  5.3980e-02, -5.0866e-02,\n",
            "         -7.1526e-03,  5.1218e-02, -1.9006e-02,  4.9270e-02, -6.1052e-03,\n",
            "         -4.8445e-02,  5.0740e-02,  6.6242e-03,  2.1872e-02,  5.4263e-04,\n",
            "          1.4086e-02, -3.2916e-02, -5.0851e-02,  2.1217e-02, -2.0046e-02,\n",
            "         -2.6388e-02, -2.2649e-02,  3.1654e-02, -1.7649e-02,  5.8297e-02,\n",
            "          3.4469e-02,  5.6848e-02, -4.9177e-03,  2.1603e-02,  5.6651e-02,\n",
            "          2.1943e-02, -2.3474e-02, -2.6443e-02, -5.6190e-02, -5.7727e-02,\n",
            "          2.4434e-02, -6.0318e-02, -2.7070e-02, -3.2727e-02, -2.5991e-02,\n",
            "         -4.7934e-02, -2.1865e-02, -4.7683e-02, -5.2380e-02, -5.5983e-02,\n",
            "         -3.8045e-02,  1.7864e-02, -3.3668e-02, -1.9249e-02, -4.4297e-02,\n",
            "          1.5201e-02, -1.0403e-02,  4.8831e-02,  5.8074e-03, -4.8868e-02,\n",
            "         -5.6740e-02, -1.3482e-02,  3.6886e-03, -2.6540e-03,  1.2653e-02,\n",
            "         -1.6432e-02, -4.8539e-02,  5.1886e-02, -2.3683e-03, -2.2822e-02,\n",
            "          3.8898e-02, -1.5969e-02,  6.2160e-02,  3.0610e-02,  3.7326e-02,\n",
            "          1.2295e-02],\n",
            "        [-1.5434e-02,  1.9559e-02,  2.0975e-02, -2.6463e-02, -5.8222e-03,\n",
            "         -5.1503e-02, -1.3175e-02, -2.2433e-02,  5.6863e-02,  2.0219e-03,\n",
            "         -4.8782e-02,  6.1406e-02,  5.2202e-02, -1.5980e-02, -2.5721e-02,\n",
            "          5.2051e-02, -3.8145e-02,  2.2668e-02,  5.8192e-02,  8.7860e-03,\n",
            "          5.6623e-02, -4.9577e-02, -5.7003e-02,  4.8556e-02, -3.8013e-02,\n",
            "         -2.5343e-02,  4.8114e-02,  1.7053e-02, -1.4471e-03,  5.6894e-02,\n",
            "          5.9726e-02, -7.2743e-03,  2.1875e-02,  9.7738e-03,  4.0981e-02,\n",
            "          3.4019e-02, -1.2905e-02,  4.1642e-02,  4.8498e-02, -3.6117e-02,\n",
            "         -2.9341e-03,  2.4605e-02, -5.7642e-03,  3.6290e-02, -7.6481e-03,\n",
            "         -9.2377e-03, -2.5396e-02,  5.1516e-02, -4.2583e-03, -5.8128e-02,\n",
            "         -2.2477e-02, -7.0306e-03,  2.3915e-02, -3.8892e-05, -5.0127e-02,\n",
            "         -1.5470e-02, -2.1184e-02, -1.2354e-02,  4.7176e-02,  1.1090e-02,\n",
            "          4.8710e-02, -3.4984e-02, -3.0356e-02,  1.3914e-02, -5.7830e-02,\n",
            "          1.0777e-02,  4.0242e-02,  5.6388e-02, -2.2476e-02, -4.1502e-03,\n",
            "         -1.1066e-02,  2.8565e-02, -3.8769e-02,  3.6678e-02,  5.8744e-02,\n",
            "          3.6354e-03, -3.9613e-02,  3.2719e-02,  4.8388e-02, -4.1181e-02,\n",
            "          4.2324e-02, -4.9720e-02,  6.1622e-02,  5.0580e-03,  3.6601e-02,\n",
            "          1.2075e-02,  3.6703e-02, -5.7193e-02,  3.1673e-02,  1.1410e-02,\n",
            "         -5.9392e-02,  6.1110e-02,  3.7297e-02, -3.2612e-02, -4.8509e-02,\n",
            "         -1.4540e-02,  4.2611e-02,  3.8478e-02,  4.6484e-02,  3.9145e-03,\n",
            "         -6.1264e-02, -1.2304e-03,  3.9255e-02,  2.2954e-02, -3.5324e-02,\n",
            "          1.8416e-02,  5.7917e-02, -3.2337e-02, -1.4302e-02, -3.1889e-02,\n",
            "         -2.5761e-02, -2.2639e-02, -2.6968e-02,  4.7702e-02,  2.5200e-02,\n",
            "         -4.0845e-02, -5.8531e-02, -1.0676e-02,  3.6177e-02,  2.5439e-02,\n",
            "         -2.1643e-02, -3.2272e-02,  4.1453e-02,  7.9663e-03, -4.0475e-03,\n",
            "         -4.9023e-02, -2.0352e-02, -3.4436e-02,  1.1333e-02, -3.2983e-02,\n",
            "          1.1117e-02,  2.4796e-02,  2.1715e-02,  2.0984e-02,  2.0484e-02,\n",
            "          4.9784e-02,  4.3908e-02,  3.0135e-02,  1.9410e-02, -6.5580e-04,\n",
            "         -2.3322e-03, -2.7575e-02,  2.9994e-02,  8.9208e-03,  5.8592e-02,\n",
            "          3.5434e-02,  3.8826e-02,  2.8078e-02,  5.9520e-02,  4.3591e-02,\n",
            "         -2.6142e-02, -7.3076e-03,  4.4380e-02,  1.9002e-02, -4.5394e-02,\n",
            "         -4.2174e-02,  5.6185e-02,  4.7495e-03, -1.1332e-02,  3.1106e-02,\n",
            "          1.6501e-02,  2.8738e-02,  4.7781e-02,  6.6359e-03, -3.2084e-02,\n",
            "         -2.8902e-02, -1.2515e-02,  1.8542e-02,  3.2498e-03,  6.1624e-02,\n",
            "          2.7777e-02, -6.1573e-02,  1.6256e-02,  6.1868e-02, -2.4912e-02,\n",
            "         -3.8617e-02, -5.7780e-02, -7.9884e-04, -4.8615e-02, -7.8966e-03,\n",
            "          6.1513e-02,  5.6146e-02, -4.9791e-02,  6.0128e-02, -2.2675e-02,\n",
            "          3.0534e-02,  2.7394e-02, -5.2073e-02, -4.2907e-02, -3.1327e-02,\n",
            "         -3.6806e-02, -9.4273e-03,  4.1642e-04,  2.7157e-02, -2.8314e-02,\n",
            "          3.1303e-02,  5.2779e-03, -6.7529e-03, -2.3269e-02, -5.7809e-02,\n",
            "         -5.3955e-02,  3.9113e-02,  4.1870e-02,  4.1951e-02,  3.2195e-02,\n",
            "          5.0391e-02, -5.1861e-04,  3.9895e-02,  3.0065e-04,  3.9918e-02,\n",
            "          1.3091e-03,  1.5203e-02,  3.0946e-02,  2.1529e-02, -3.1377e-02,\n",
            "          4.9128e-03,  1.6725e-02, -2.6229e-02,  5.7918e-02, -3.0878e-03,\n",
            "          2.2054e-02,  1.3156e-02,  5.1221e-02,  2.1734e-02,  3.6364e-02,\n",
            "          6.3182e-03, -4.9644e-02,  2.8317e-03,  7.5937e-03,  4.7543e-02,\n",
            "          3.1049e-02, -2.3425e-02, -3.5390e-02, -1.8549e-02,  4.0983e-02,\n",
            "         -5.1699e-02, -6.0915e-02,  1.1004e-03,  4.1947e-02, -2.3651e-02,\n",
            "         -3.4721e-02, -5.7033e-02, -5.3665e-02,  5.0306e-02, -5.2902e-02,\n",
            "          2.7227e-02,  1.3220e-02,  5.5944e-02,  7.2274e-03,  4.2232e-03,\n",
            "          6.0653e-02,  6.2224e-02,  4.6510e-02, -5.8565e-02, -4.5283e-02,\n",
            "         -9.9762e-03]], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 2.bias | Size: torch.Size([64]) | Values : tensor([0.0617, 0.0347], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 4.weight | Size: torch.Size([1, 64]) | Values : tensor([[-0.1154,  0.0376, -0.0418, -0.0087, -0.0273,  0.1049,  0.1035,  0.0972,\n",
            "         -0.1249, -0.0927, -0.0464,  0.0672,  0.0229,  0.0425,  0.0108, -0.0213,\n",
            "         -0.0163,  0.0516,  0.0023, -0.0431,  0.0239, -0.0065, -0.0663,  0.0870,\n",
            "         -0.0213,  0.0302, -0.1068, -0.0150, -0.0798, -0.0087,  0.0250, -0.0321,\n",
            "          0.0719, -0.1244, -0.0612,  0.0951,  0.1227,  0.0868,  0.0601, -0.0596,\n",
            "         -0.1084,  0.1054, -0.0270, -0.0765, -0.0504, -0.0679, -0.0223,  0.1163,\n",
            "          0.0052,  0.1174, -0.0417, -0.1124, -0.0535, -0.0071,  0.1129,  0.1013,\n",
            "          0.0902, -0.0242, -0.0722, -0.0932, -0.0955,  0.0598, -0.0363, -0.0081]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 4.bias | Size: torch.Size([1]) | Values : tensor([-0.0440], grad_fn=<SliceBackward>) \n",
            "\n",
            "Epochs: 1,running_loss: 28.0,accuracy: 72.0\n",
            "Layer: 0.weight | Size: torch.Size([256, 23]) | Values : tensor([[ 0.0334,  0.1095,  0.0535, -0.1839,  0.1524,  0.1039,  0.1624,  0.0865,\n",
            "          0.0447, -0.1215, -0.0344, -0.0420, -0.0399,  0.1605,  0.1428, -0.1647,\n",
            "          0.0978,  0.0232, -0.1973, -0.0765,  0.0285, -0.1268,  0.0600],\n",
            "        [ 0.0876, -0.0514, -0.1535,  0.0975, -0.1003, -0.0278, -0.1122,  0.1575,\n",
            "          0.1764,  0.0404,  0.1649, -0.1364, -0.1664,  0.0460,  0.0958,  0.0899,\n",
            "         -0.1433,  0.0765,  0.0801, -0.0380,  0.1858,  0.0374, -0.1264]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 0.bias | Size: torch.Size([256]) | Values : tensor([-0.0540,  0.0107], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 2.weight | Size: torch.Size([64, 256]) | Values : tensor([[-5.8623e-02,  3.1298e-02,  4.8088e-02,  2.0751e-02,  4.9323e-03,\n",
            "         -4.9633e-02, -4.1430e-02, -6.0132e-02, -1.2477e-02,  3.0073e-03,\n",
            "          1.6044e-02,  2.9084e-02,  4.6019e-02, -2.6304e-02, -8.3184e-03,\n",
            "          3.1041e-02, -1.8753e-02, -3.8682e-02,  5.8958e-02, -7.9693e-03,\n",
            "         -4.1230e-02, -3.6553e-02, -4.3637e-02,  6.0021e-02, -3.2571e-02,\n",
            "         -6.8401e-03,  9.3616e-03, -3.8027e-02,  5.4041e-03, -1.8520e-02,\n",
            "         -3.5970e-02, -3.4900e-02,  4.5161e-03,  5.4626e-02,  4.1976e-02,\n",
            "         -1.2721e-03,  2.5226e-02,  6.1520e-02,  1.8663e-02, -4.6688e-02,\n",
            "          2.7044e-02,  2.3453e-02, -2.8371e-02,  1.7704e-02,  1.8997e-02,\n",
            "         -2.0838e-02, -2.5912e-02, -4.0489e-02, -3.0490e-02,  1.6585e-02,\n",
            "          7.5735e-03,  5.2090e-02, -3.0198e-02, -4.9985e-02, -5.5186e-02,\n",
            "         -2.2005e-02,  5.8541e-02,  4.9938e-02,  1.6151e-02,  2.7960e-02,\n",
            "         -3.5706e-02,  3.6234e-02, -1.9187e-02,  2.3055e-02, -5.5042e-02,\n",
            "         -2.0912e-02, -4.8102e-02, -3.9940e-02, -1.9925e-02,  3.9085e-02,\n",
            "         -1.1768e-03, -4.3147e-03, -1.0583e-02, -2.5338e-02, -4.3769e-02,\n",
            "          5.0673e-02,  3.8777e-02, -3.8908e-02, -1.9873e-02, -1.0981e-02,\n",
            "         -4.0490e-02,  3.8214e-02,  4.1300e-02,  3.2931e-02, -2.8845e-02,\n",
            "          4.7091e-02,  3.1983e-02,  4.7104e-02, -2.3783e-02, -5.8941e-02,\n",
            "          5.9361e-02,  5.8680e-02,  5.4543e-03, -1.7428e-02,  1.2293e-02,\n",
            "          1.6191e-02,  4.7547e-02, -1.7308e-02, -2.7145e-02,  2.3784e-02,\n",
            "          4.3072e-02,  3.0716e-03,  4.4007e-02,  1.3740e-03,  6.1135e-03,\n",
            "         -5.5594e-02, -2.6177e-02,  2.7192e-03, -4.3180e-02,  8.4083e-03,\n",
            "         -4.3120e-02, -8.9889e-03, -3.3969e-03, -5.1247e-02,  1.3959e-02,\n",
            "         -3.9742e-02, -3.3993e-02,  8.1451e-03, -1.2535e-02,  4.8600e-02,\n",
            "         -3.1399e-02, -1.5234e-02,  5.2034e-02, -1.9631e-02, -2.9294e-03,\n",
            "          7.7736e-03,  2.8451e-02, -5.8729e-02,  2.3520e-03,  5.7636e-02,\n",
            "          2.4307e-02,  6.0702e-02, -6.2082e-02,  5.2472e-02,  4.9470e-03,\n",
            "          3.2662e-02,  4.2935e-02,  3.5921e-02,  4.3097e-02, -3.2475e-02,\n",
            "         -4.4825e-03,  1.4024e-02, -2.2177e-02,  1.4418e-02, -2.1048e-02,\n",
            "          2.0322e-02, -9.5908e-03,  4.1660e-03, -4.2933e-02,  5.8732e-02,\n",
            "         -1.0700e-02, -1.7704e-03,  3.8965e-02,  1.4623e-02,  7.0755e-03,\n",
            "          3.8006e-02, -4.0189e-02, -6.0178e-02,  3.5217e-02,  2.6748e-02,\n",
            "         -2.0527e-02, -1.1958e-03, -5.4087e-02, -3.5670e-02,  4.1478e-02,\n",
            "          1.7810e-03,  5.2662e-02,  5.0116e-02,  4.6311e-02, -1.0135e-03,\n",
            "         -3.4414e-02,  5.4454e-02,  1.9309e-02,  1.3896e-02, -1.4773e-02,\n",
            "          6.1425e-04, -2.1494e-03,  2.4583e-02,  5.1489e-02, -3.6403e-02,\n",
            "         -3.5545e-02,  4.3674e-02,  1.4799e-02, -2.7804e-02,  2.5681e-02,\n",
            "         -2.0811e-02, -5.5847e-02, -7.7742e-04,  5.3980e-02, -5.0866e-02,\n",
            "         -7.1526e-03,  5.1218e-02, -1.9006e-02,  4.9270e-02, -6.1052e-03,\n",
            "         -4.8445e-02,  5.0740e-02,  6.6242e-03,  2.1872e-02,  5.4263e-04,\n",
            "          1.4086e-02, -3.2916e-02, -5.0851e-02,  2.1217e-02, -2.0046e-02,\n",
            "         -2.6388e-02, -2.2649e-02,  3.1654e-02, -1.7649e-02,  5.8297e-02,\n",
            "          3.4469e-02,  5.6848e-02, -4.9177e-03,  2.1603e-02,  5.6651e-02,\n",
            "          2.1943e-02, -2.3474e-02, -2.6443e-02, -5.6190e-02, -5.7727e-02,\n",
            "          2.4434e-02, -6.0318e-02, -2.7070e-02, -3.2727e-02, -2.5991e-02,\n",
            "         -4.7934e-02, -2.1865e-02, -4.7683e-02, -5.2380e-02, -5.5983e-02,\n",
            "         -3.8045e-02,  1.7864e-02, -3.3668e-02, -1.9249e-02, -4.4297e-02,\n",
            "          1.5201e-02, -1.0403e-02,  4.8831e-02,  5.8074e-03, -4.8868e-02,\n",
            "         -5.6740e-02, -1.3482e-02,  3.6886e-03, -2.6540e-03,  1.2653e-02,\n",
            "         -1.6432e-02, -4.8539e-02,  5.1886e-02, -2.3683e-03, -2.2822e-02,\n",
            "          3.8898e-02, -1.5969e-02,  6.2160e-02,  3.0610e-02,  3.7326e-02,\n",
            "          1.2295e-02],\n",
            "        [-1.5434e-02,  1.9559e-02,  2.0975e-02, -2.6463e-02, -5.8222e-03,\n",
            "         -5.1503e-02, -1.3175e-02, -2.2433e-02,  5.6863e-02,  2.0219e-03,\n",
            "         -4.8782e-02,  6.1406e-02,  5.2202e-02, -1.5980e-02, -2.5721e-02,\n",
            "          5.2051e-02, -3.8145e-02,  2.2668e-02,  5.8192e-02,  8.7860e-03,\n",
            "          5.6623e-02, -4.9577e-02, -5.7003e-02,  4.8556e-02, -3.8013e-02,\n",
            "         -2.5343e-02,  4.8114e-02,  1.7053e-02, -1.4471e-03,  5.6894e-02,\n",
            "          5.9726e-02, -7.2743e-03,  2.1875e-02,  9.7738e-03,  4.0981e-02,\n",
            "          3.4019e-02, -1.2905e-02,  4.1642e-02,  4.8498e-02, -3.6117e-02,\n",
            "         -2.9341e-03,  2.4605e-02, -5.7642e-03,  3.6290e-02, -7.6481e-03,\n",
            "         -9.2377e-03, -2.5396e-02,  5.1516e-02, -4.2583e-03, -5.8128e-02,\n",
            "         -2.2477e-02, -7.0306e-03,  2.3915e-02, -3.8892e-05, -5.0127e-02,\n",
            "         -1.5470e-02, -2.1184e-02, -1.2354e-02,  4.7176e-02,  1.1090e-02,\n",
            "          4.8710e-02, -3.4984e-02, -3.0356e-02,  1.3914e-02, -5.7830e-02,\n",
            "          1.0777e-02,  4.0242e-02,  5.6388e-02, -2.2476e-02, -4.1502e-03,\n",
            "         -1.1066e-02,  2.8565e-02, -3.8769e-02,  3.6678e-02,  5.8744e-02,\n",
            "          3.6354e-03, -3.9613e-02,  3.2719e-02,  4.8388e-02, -4.1181e-02,\n",
            "          4.2324e-02, -4.9720e-02,  6.1622e-02,  5.0580e-03,  3.6601e-02,\n",
            "          1.2075e-02,  3.6703e-02, -5.7193e-02,  3.1673e-02,  1.1410e-02,\n",
            "         -5.9392e-02,  6.1110e-02,  3.7297e-02, -3.2612e-02, -4.8509e-02,\n",
            "         -1.4540e-02,  4.2611e-02,  3.8478e-02,  4.6484e-02,  3.9145e-03,\n",
            "         -6.1264e-02, -1.2304e-03,  3.9255e-02,  2.2954e-02, -3.5324e-02,\n",
            "          1.8416e-02,  5.7917e-02, -3.2337e-02, -1.4302e-02, -3.1889e-02,\n",
            "         -2.5761e-02, -2.2639e-02, -2.6968e-02,  4.7702e-02,  2.5200e-02,\n",
            "         -4.0845e-02, -5.8531e-02, -1.0676e-02,  3.6177e-02,  2.5439e-02,\n",
            "         -2.1643e-02, -3.2272e-02,  4.1453e-02,  7.9663e-03, -4.0475e-03,\n",
            "         -4.9023e-02, -2.0352e-02, -3.4436e-02,  1.1333e-02, -3.2983e-02,\n",
            "          1.1117e-02,  2.4796e-02,  2.1715e-02,  2.0984e-02,  2.0484e-02,\n",
            "          4.9784e-02,  4.3908e-02,  3.0135e-02,  1.9410e-02, -6.5580e-04,\n",
            "         -2.3322e-03, -2.7575e-02,  2.9994e-02,  8.9208e-03,  5.8592e-02,\n",
            "          3.5434e-02,  3.8826e-02,  2.8078e-02,  5.9520e-02,  4.3591e-02,\n",
            "         -2.6142e-02, -7.3076e-03,  4.4380e-02,  1.9002e-02, -4.5394e-02,\n",
            "         -4.2174e-02,  5.6185e-02,  4.7495e-03, -1.1332e-02,  3.1106e-02,\n",
            "          1.6501e-02,  2.8738e-02,  4.7781e-02,  6.6359e-03, -3.2084e-02,\n",
            "         -2.8902e-02, -1.2515e-02,  1.8542e-02,  3.2498e-03,  6.1624e-02,\n",
            "          2.7777e-02, -6.1573e-02,  1.6256e-02,  6.1868e-02, -2.4912e-02,\n",
            "         -3.8617e-02, -5.7780e-02, -7.9884e-04, -4.8615e-02, -7.8966e-03,\n",
            "          6.1513e-02,  5.6146e-02, -4.9791e-02,  6.0128e-02, -2.2675e-02,\n",
            "          3.0534e-02,  2.7394e-02, -5.2073e-02, -4.2907e-02, -3.1327e-02,\n",
            "         -3.6806e-02, -9.4273e-03,  4.1642e-04,  2.7157e-02, -2.8314e-02,\n",
            "          3.1303e-02,  5.2779e-03, -6.7529e-03, -2.3269e-02, -5.7809e-02,\n",
            "         -5.3955e-02,  3.9113e-02,  4.1870e-02,  4.1951e-02,  3.2195e-02,\n",
            "          5.0391e-02, -5.1861e-04,  3.9895e-02,  3.0065e-04,  3.9918e-02,\n",
            "          1.3091e-03,  1.5203e-02,  3.0946e-02,  2.1529e-02, -3.1377e-02,\n",
            "          4.9128e-03,  1.6725e-02, -2.6229e-02,  5.7918e-02, -3.0878e-03,\n",
            "          2.2054e-02,  1.3156e-02,  5.1221e-02,  2.1734e-02,  3.6364e-02,\n",
            "          6.3182e-03, -4.9644e-02,  2.8317e-03,  7.5937e-03,  4.7543e-02,\n",
            "          3.1049e-02, -2.3425e-02, -3.5390e-02, -1.8549e-02,  4.0983e-02,\n",
            "         -5.1699e-02, -6.0915e-02,  1.1004e-03,  4.1947e-02, -2.3651e-02,\n",
            "         -3.4721e-02, -5.7033e-02, -5.3665e-02,  5.0306e-02, -5.2902e-02,\n",
            "          2.7227e-02,  1.3220e-02,  5.5944e-02,  7.2274e-03,  4.2232e-03,\n",
            "          6.0653e-02,  6.2224e-02,  4.6510e-02, -5.8565e-02, -4.5283e-02,\n",
            "         -9.9762e-03]], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 2.bias | Size: torch.Size([64]) | Values : tensor([0.0617, 0.0347], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 4.weight | Size: torch.Size([1, 64]) | Values : tensor([[-0.1154,  0.0376, -0.0418, -0.0087, -0.0273,  0.1049,  0.1035,  0.0972,\n",
            "         -0.1249, -0.0927, -0.0464,  0.0672,  0.0229,  0.0425,  0.0108, -0.0213,\n",
            "         -0.0163,  0.0516,  0.0023, -0.0431,  0.0239, -0.0065, -0.0663,  0.0870,\n",
            "         -0.0213,  0.0302, -0.1068, -0.0150, -0.0798, -0.0087,  0.0250, -0.0321,\n",
            "          0.0719, -0.1244, -0.0612,  0.0951,  0.1227,  0.0868,  0.0601, -0.0596,\n",
            "         -0.1084,  0.1054, -0.0270, -0.0765, -0.0504, -0.0679, -0.0223,  0.1163,\n",
            "          0.0052,  0.1174, -0.0417, -0.1124, -0.0535, -0.0071,  0.1129,  0.1013,\n",
            "          0.0902, -0.0242, -0.0722, -0.0932, -0.0955,  0.0598, -0.0363, -0.0081]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 4.bias | Size: torch.Size([1]) | Values : tensor([-0.0440], grad_fn=<SliceBackward>) \n",
            "\n",
            "Epochs: 2,running_loss: 25.0,accuracy: 75.0\n",
            "Layer: 0.weight | Size: torch.Size([256, 23]) | Values : tensor([[ 0.0334,  0.1095,  0.0535, -0.1839,  0.1524,  0.1039,  0.1624,  0.0865,\n",
            "          0.0447, -0.1215, -0.0344, -0.0420, -0.0399,  0.1605,  0.1428, -0.1647,\n",
            "          0.0978,  0.0232, -0.1973, -0.0765,  0.0285, -0.1268,  0.0600],\n",
            "        [ 0.0876, -0.0514, -0.1535,  0.0975, -0.1003, -0.0278, -0.1122,  0.1575,\n",
            "          0.1764,  0.0404,  0.1649, -0.1364, -0.1664,  0.0460,  0.0958,  0.0899,\n",
            "         -0.1433,  0.0765,  0.0801, -0.0380,  0.1858,  0.0374, -0.1264]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 0.bias | Size: torch.Size([256]) | Values : tensor([-0.0540,  0.0107], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 2.weight | Size: torch.Size([64, 256]) | Values : tensor([[-5.8623e-02,  3.1298e-02,  4.8088e-02,  2.0751e-02,  4.9323e-03,\n",
            "         -4.9633e-02, -4.1430e-02, -6.0132e-02, -1.2477e-02,  3.0073e-03,\n",
            "          1.6044e-02,  2.9084e-02,  4.6019e-02, -2.6304e-02, -8.3184e-03,\n",
            "          3.1041e-02, -1.8753e-02, -3.8682e-02,  5.8958e-02, -7.9693e-03,\n",
            "         -4.1230e-02, -3.6553e-02, -4.3637e-02,  6.0021e-02, -3.2571e-02,\n",
            "         -6.8401e-03,  9.3616e-03, -3.8027e-02,  5.4041e-03, -1.8520e-02,\n",
            "         -3.5970e-02, -3.4900e-02,  4.5161e-03,  5.4626e-02,  4.1976e-02,\n",
            "         -1.2721e-03,  2.5226e-02,  6.1520e-02,  1.8663e-02, -4.6688e-02,\n",
            "          2.7044e-02,  2.3453e-02, -2.8371e-02,  1.7704e-02,  1.8997e-02,\n",
            "         -2.0838e-02, -2.5912e-02, -4.0489e-02, -3.0490e-02,  1.6585e-02,\n",
            "          7.5735e-03,  5.2090e-02, -3.0198e-02, -4.9985e-02, -5.5186e-02,\n",
            "         -2.2005e-02,  5.8541e-02,  4.9938e-02,  1.6151e-02,  2.7960e-02,\n",
            "         -3.5706e-02,  3.6234e-02, -1.9187e-02,  2.3055e-02, -5.5042e-02,\n",
            "         -2.0912e-02, -4.8102e-02, -3.9940e-02, -1.9925e-02,  3.9085e-02,\n",
            "         -1.1768e-03, -4.3147e-03, -1.0583e-02, -2.5338e-02, -4.3769e-02,\n",
            "          5.0673e-02,  3.8777e-02, -3.8908e-02, -1.9873e-02, -1.0981e-02,\n",
            "         -4.0490e-02,  3.8214e-02,  4.1300e-02,  3.2931e-02, -2.8845e-02,\n",
            "          4.7091e-02,  3.1983e-02,  4.7104e-02, -2.3783e-02, -5.8941e-02,\n",
            "          5.9361e-02,  5.8680e-02,  5.4543e-03, -1.7428e-02,  1.2293e-02,\n",
            "          1.6191e-02,  4.7547e-02, -1.7308e-02, -2.7145e-02,  2.3784e-02,\n",
            "          4.3072e-02,  3.0716e-03,  4.4007e-02,  1.3740e-03,  6.1135e-03,\n",
            "         -5.5594e-02, -2.6177e-02,  2.7192e-03, -4.3180e-02,  8.4083e-03,\n",
            "         -4.3120e-02, -8.9889e-03, -3.3969e-03, -5.1247e-02,  1.3959e-02,\n",
            "         -3.9742e-02, -3.3993e-02,  8.1451e-03, -1.2535e-02,  4.8600e-02,\n",
            "         -3.1399e-02, -1.5234e-02,  5.2034e-02, -1.9631e-02, -2.9294e-03,\n",
            "          7.7736e-03,  2.8451e-02, -5.8729e-02,  2.3520e-03,  5.7636e-02,\n",
            "          2.4307e-02,  6.0702e-02, -6.2082e-02,  5.2472e-02,  4.9470e-03,\n",
            "          3.2662e-02,  4.2935e-02,  3.5921e-02,  4.3097e-02, -3.2475e-02,\n",
            "         -4.4825e-03,  1.4024e-02, -2.2177e-02,  1.4418e-02, -2.1048e-02,\n",
            "          2.0322e-02, -9.5908e-03,  4.1660e-03, -4.2933e-02,  5.8732e-02,\n",
            "         -1.0700e-02, -1.7704e-03,  3.8965e-02,  1.4623e-02,  7.0755e-03,\n",
            "          3.8006e-02, -4.0189e-02, -6.0178e-02,  3.5217e-02,  2.6748e-02,\n",
            "         -2.0527e-02, -1.1958e-03, -5.4087e-02, -3.5670e-02,  4.1478e-02,\n",
            "          1.7810e-03,  5.2662e-02,  5.0116e-02,  4.6311e-02, -1.0135e-03,\n",
            "         -3.4414e-02,  5.4454e-02,  1.9309e-02,  1.3896e-02, -1.4773e-02,\n",
            "          6.1425e-04, -2.1494e-03,  2.4583e-02,  5.1489e-02, -3.6403e-02,\n",
            "         -3.5545e-02,  4.3674e-02,  1.4799e-02, -2.7804e-02,  2.5681e-02,\n",
            "         -2.0811e-02, -5.5847e-02, -7.7742e-04,  5.3980e-02, -5.0866e-02,\n",
            "         -7.1526e-03,  5.1218e-02, -1.9006e-02,  4.9270e-02, -6.1052e-03,\n",
            "         -4.8445e-02,  5.0740e-02,  6.6242e-03,  2.1872e-02,  5.4263e-04,\n",
            "          1.4086e-02, -3.2916e-02, -5.0851e-02,  2.1217e-02, -2.0046e-02,\n",
            "         -2.6388e-02, -2.2649e-02,  3.1654e-02, -1.7649e-02,  5.8297e-02,\n",
            "          3.4469e-02,  5.6848e-02, -4.9177e-03,  2.1603e-02,  5.6651e-02,\n",
            "          2.1943e-02, -2.3474e-02, -2.6443e-02, -5.6190e-02, -5.7727e-02,\n",
            "          2.4434e-02, -6.0318e-02, -2.7070e-02, -3.2727e-02, -2.5991e-02,\n",
            "         -4.7934e-02, -2.1865e-02, -4.7683e-02, -5.2380e-02, -5.5983e-02,\n",
            "         -3.8045e-02,  1.7864e-02, -3.3668e-02, -1.9249e-02, -4.4297e-02,\n",
            "          1.5201e-02, -1.0403e-02,  4.8831e-02,  5.8074e-03, -4.8868e-02,\n",
            "         -5.6740e-02, -1.3482e-02,  3.6886e-03, -2.6540e-03,  1.2653e-02,\n",
            "         -1.6432e-02, -4.8539e-02,  5.1886e-02, -2.3683e-03, -2.2822e-02,\n",
            "          3.8898e-02, -1.5969e-02,  6.2160e-02,  3.0610e-02,  3.7326e-02,\n",
            "          1.2295e-02],\n",
            "        [-1.5434e-02,  1.9559e-02,  2.0975e-02, -2.6463e-02, -5.8222e-03,\n",
            "         -5.1503e-02, -1.3175e-02, -2.2433e-02,  5.6863e-02,  2.0219e-03,\n",
            "         -4.8782e-02,  6.1406e-02,  5.2202e-02, -1.5980e-02, -2.5721e-02,\n",
            "          5.2051e-02, -3.8145e-02,  2.2668e-02,  5.8192e-02,  8.7860e-03,\n",
            "          5.6623e-02, -4.9577e-02, -5.7003e-02,  4.8556e-02, -3.8013e-02,\n",
            "         -2.5343e-02,  4.8114e-02,  1.7053e-02, -1.4471e-03,  5.6894e-02,\n",
            "          5.9726e-02, -7.2743e-03,  2.1875e-02,  9.7738e-03,  4.0981e-02,\n",
            "          3.4019e-02, -1.2905e-02,  4.1642e-02,  4.8498e-02, -3.6117e-02,\n",
            "         -2.9341e-03,  2.4605e-02, -5.7642e-03,  3.6290e-02, -7.6481e-03,\n",
            "         -9.2377e-03, -2.5396e-02,  5.1516e-02, -4.2583e-03, -5.8128e-02,\n",
            "         -2.2477e-02, -7.0306e-03,  2.3915e-02, -3.8892e-05, -5.0127e-02,\n",
            "         -1.5470e-02, -2.1184e-02, -1.2354e-02,  4.7176e-02,  1.1090e-02,\n",
            "          4.8710e-02, -3.4984e-02, -3.0356e-02,  1.3914e-02, -5.7830e-02,\n",
            "          1.0777e-02,  4.0242e-02,  5.6388e-02, -2.2476e-02, -4.1502e-03,\n",
            "         -1.1066e-02,  2.8565e-02, -3.8769e-02,  3.6678e-02,  5.8744e-02,\n",
            "          3.6354e-03, -3.9613e-02,  3.2719e-02,  4.8388e-02, -4.1181e-02,\n",
            "          4.2324e-02, -4.9720e-02,  6.1622e-02,  5.0580e-03,  3.6601e-02,\n",
            "          1.2075e-02,  3.6703e-02, -5.7193e-02,  3.1673e-02,  1.1410e-02,\n",
            "         -5.9392e-02,  6.1110e-02,  3.7297e-02, -3.2612e-02, -4.8509e-02,\n",
            "         -1.4540e-02,  4.2611e-02,  3.8478e-02,  4.6484e-02,  3.9145e-03,\n",
            "         -6.1264e-02, -1.2304e-03,  3.9255e-02,  2.2954e-02, -3.5324e-02,\n",
            "          1.8416e-02,  5.7917e-02, -3.2337e-02, -1.4302e-02, -3.1889e-02,\n",
            "         -2.5761e-02, -2.2639e-02, -2.6968e-02,  4.7702e-02,  2.5200e-02,\n",
            "         -4.0845e-02, -5.8531e-02, -1.0676e-02,  3.6177e-02,  2.5439e-02,\n",
            "         -2.1643e-02, -3.2272e-02,  4.1453e-02,  7.9663e-03, -4.0475e-03,\n",
            "         -4.9023e-02, -2.0352e-02, -3.4436e-02,  1.1333e-02, -3.2983e-02,\n",
            "          1.1117e-02,  2.4796e-02,  2.1715e-02,  2.0984e-02,  2.0484e-02,\n",
            "          4.9784e-02,  4.3908e-02,  3.0135e-02,  1.9410e-02, -6.5580e-04,\n",
            "         -2.3322e-03, -2.7575e-02,  2.9994e-02,  8.9208e-03,  5.8592e-02,\n",
            "          3.5434e-02,  3.8826e-02,  2.8078e-02,  5.9520e-02,  4.3591e-02,\n",
            "         -2.6142e-02, -7.3076e-03,  4.4380e-02,  1.9002e-02, -4.5394e-02,\n",
            "         -4.2174e-02,  5.6185e-02,  4.7495e-03, -1.1332e-02,  3.1106e-02,\n",
            "          1.6501e-02,  2.8738e-02,  4.7781e-02,  6.6359e-03, -3.2084e-02,\n",
            "         -2.8902e-02, -1.2515e-02,  1.8542e-02,  3.2498e-03,  6.1624e-02,\n",
            "          2.7777e-02, -6.1573e-02,  1.6256e-02,  6.1868e-02, -2.4912e-02,\n",
            "         -3.8617e-02, -5.7780e-02, -7.9884e-04, -4.8615e-02, -7.8966e-03,\n",
            "          6.1513e-02,  5.6146e-02, -4.9791e-02,  6.0128e-02, -2.2675e-02,\n",
            "          3.0534e-02,  2.7394e-02, -5.2073e-02, -4.2907e-02, -3.1327e-02,\n",
            "         -3.6806e-02, -9.4273e-03,  4.1642e-04,  2.7157e-02, -2.8314e-02,\n",
            "          3.1303e-02,  5.2779e-03, -6.7529e-03, -2.3269e-02, -5.7809e-02,\n",
            "         -5.3955e-02,  3.9113e-02,  4.1870e-02,  4.1951e-02,  3.2195e-02,\n",
            "          5.0391e-02, -5.1861e-04,  3.9895e-02,  3.0065e-04,  3.9918e-02,\n",
            "          1.3091e-03,  1.5203e-02,  3.0946e-02,  2.1529e-02, -3.1377e-02,\n",
            "          4.9128e-03,  1.6725e-02, -2.6229e-02,  5.7918e-02, -3.0878e-03,\n",
            "          2.2054e-02,  1.3156e-02,  5.1221e-02,  2.1734e-02,  3.6364e-02,\n",
            "          6.3182e-03, -4.9644e-02,  2.8317e-03,  7.5937e-03,  4.7543e-02,\n",
            "          3.1049e-02, -2.3425e-02, -3.5390e-02, -1.8549e-02,  4.0983e-02,\n",
            "         -5.1699e-02, -6.0915e-02,  1.1004e-03,  4.1947e-02, -2.3651e-02,\n",
            "         -3.4721e-02, -5.7033e-02, -5.3665e-02,  5.0306e-02, -5.2902e-02,\n",
            "          2.7227e-02,  1.3220e-02,  5.5944e-02,  7.2274e-03,  4.2232e-03,\n",
            "          6.0653e-02,  6.2224e-02,  4.6510e-02, -5.8565e-02, -4.5283e-02,\n",
            "         -9.9762e-03]], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 2.bias | Size: torch.Size([64]) | Values : tensor([0.0617, 0.0347], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 4.weight | Size: torch.Size([1, 64]) | Values : tensor([[-0.1154,  0.0376, -0.0418, -0.0087, -0.0273,  0.1049,  0.1035,  0.0972,\n",
            "         -0.1249, -0.0927, -0.0464,  0.0672,  0.0229,  0.0425,  0.0108, -0.0213,\n",
            "         -0.0163,  0.0516,  0.0023, -0.0431,  0.0239, -0.0065, -0.0663,  0.0870,\n",
            "         -0.0213,  0.0302, -0.1068, -0.0150, -0.0798, -0.0087,  0.0250, -0.0321,\n",
            "          0.0719, -0.1244, -0.0612,  0.0951,  0.1227,  0.0868,  0.0601, -0.0596,\n",
            "         -0.1084,  0.1054, -0.0270, -0.0765, -0.0504, -0.0679, -0.0223,  0.1163,\n",
            "          0.0052,  0.1174, -0.0417, -0.1124, -0.0535, -0.0071,  0.1129,  0.1013,\n",
            "          0.0902, -0.0242, -0.0722, -0.0932, -0.0955,  0.0598, -0.0363, -0.0081]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 4.bias | Size: torch.Size([1]) | Values : tensor([-0.0440], grad_fn=<SliceBackward>) \n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZKGhw_mTRorj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "print(\"Model structure: \", model, \"\\n\\n\")\r\n",
        "\r\n",
        "for name, param in model.named_parameters():\r\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure:  Sequential(\n",
            "  (0): Linear(in_features=23, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (5): LogSoftmax(dim=1)\n",
            ") \n",
            "\n",
            "\n",
            "Layer: 0.weight | Size: torch.Size([256, 23]) | Values : tensor([[ 0.0593,  0.0226,  0.0942,  0.1355, -0.0427,  0.1051,  0.0934,  0.0163,\n",
            "          0.1207, -0.1313,  0.1631, -0.1723, -0.0658,  0.0285, -0.1342,  0.2016,\n",
            "         -0.0523,  0.0472,  0.0865,  0.1643, -0.0312, -0.1507,  0.0623],\n",
            "        [-0.1667,  0.0112,  0.0019,  0.0741, -0.1828, -0.0505, -0.0544,  0.0950,\n",
            "          0.0218,  0.0619,  0.1540,  0.0446,  0.0944, -0.1200, -0.0711,  0.0309,\n",
            "          0.1852, -0.2000,  0.1757,  0.0867, -0.0191, -0.1002,  0.1698]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 0.bias | Size: torch.Size([256]) | Values : tensor([0.1133, 0.1995], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 2.weight | Size: torch.Size([64, 256]) | Values : tensor([[-0.0237, -0.0375, -0.0231,  0.0014, -0.0602, -0.0553,  0.0338, -0.0215,\n",
            "          0.0073, -0.0319,  0.0574,  0.0382, -0.0251, -0.0579, -0.0551, -0.0200,\n",
            "          0.0455,  0.0198,  0.0352, -0.0154,  0.0573, -0.0592,  0.0470, -0.0582,\n",
            "          0.0071, -0.0426,  0.0277, -0.0118, -0.0363,  0.0419, -0.0074,  0.0242,\n",
            "         -0.0400, -0.0560, -0.0399, -0.0498,  0.0337,  0.0251,  0.0548,  0.0111,\n",
            "          0.0462, -0.0267, -0.0221,  0.0289,  0.0478,  0.0370,  0.0563,  0.0088,\n",
            "         -0.0608, -0.0275,  0.0080, -0.0375, -0.0262,  0.0612,  0.0525, -0.0136,\n",
            "          0.0282,  0.0008,  0.0099, -0.0361,  0.0440,  0.0388, -0.0251,  0.0040,\n",
            "         -0.0276,  0.0149, -0.0355,  0.0483, -0.0245, -0.0562,  0.0018, -0.0031,\n",
            "          0.0588, -0.0023,  0.0327,  0.0487,  0.0064,  0.0371,  0.0446,  0.0424,\n",
            "         -0.0197, -0.0475,  0.0350, -0.0595, -0.0043, -0.0623, -0.0290,  0.0216,\n",
            "          0.0547, -0.0608,  0.0373, -0.0457, -0.0578, -0.0166, -0.0438, -0.0460,\n",
            "         -0.0276,  0.0423,  0.0486, -0.0055,  0.0456, -0.0158,  0.0586, -0.0570,\n",
            "          0.0136,  0.0596, -0.0254, -0.0180,  0.0418, -0.0010,  0.0026, -0.0286,\n",
            "         -0.0491, -0.0059,  0.0313,  0.0410,  0.0240, -0.0315, -0.0208,  0.0053,\n",
            "         -0.0366,  0.0476, -0.0047,  0.0265,  0.0237, -0.0377,  0.0443, -0.0548,\n",
            "         -0.0502,  0.0267,  0.0591,  0.0234,  0.0594, -0.0481,  0.0551,  0.0580,\n",
            "          0.0418, -0.0033, -0.0384, -0.0159,  0.0449, -0.0471, -0.0262,  0.0602,\n",
            "          0.0125, -0.0285, -0.0606, -0.0021,  0.0396,  0.0365, -0.0121,  0.0252,\n",
            "          0.0067, -0.0478, -0.0021, -0.0415,  0.0204,  0.0128, -0.0245,  0.0207,\n",
            "         -0.0490,  0.0165,  0.0036,  0.0600, -0.0483,  0.0371,  0.0178,  0.0244,\n",
            "          0.0511, -0.0013,  0.0300, -0.0549, -0.0043,  0.0068,  0.0254,  0.0410,\n",
            "         -0.0015, -0.0435,  0.0189, -0.0339, -0.0023,  0.0157, -0.0027, -0.0073,\n",
            "         -0.0453, -0.0154,  0.0184, -0.0134,  0.0251,  0.0457, -0.0344, -0.0579,\n",
            "         -0.0417, -0.0603,  0.0021, -0.0476,  0.0584,  0.0071, -0.0551,  0.0473,\n",
            "          0.0065, -0.0201,  0.0121, -0.0453,  0.0589, -0.0186,  0.0397,  0.0559,\n",
            "         -0.0427,  0.0217, -0.0562,  0.0088,  0.0400,  0.0303, -0.0525,  0.0029,\n",
            "         -0.0546, -0.0189,  0.0134, -0.0579, -0.0298,  0.0031, -0.0184, -0.0043,\n",
            "          0.0330, -0.0560,  0.0369, -0.0167,  0.0247,  0.0435, -0.0093,  0.0553,\n",
            "         -0.0397, -0.0047, -0.0514, -0.0495,  0.0060, -0.0307, -0.0075,  0.0494,\n",
            "          0.0153,  0.0023, -0.0402, -0.0457,  0.0359,  0.0272, -0.0513,  0.0419,\n",
            "         -0.0230, -0.0087, -0.0008, -0.0101,  0.0444,  0.0559,  0.0260,  0.0442],\n",
            "        [ 0.0448, -0.0496, -0.0374, -0.0436,  0.0285, -0.0328,  0.0396, -0.0357,\n",
            "         -0.0386,  0.0565, -0.0598,  0.0543,  0.0243,  0.0256, -0.0406, -0.0421,\n",
            "         -0.0428, -0.0282, -0.0215,  0.0619,  0.0364, -0.0228, -0.0248,  0.0077,\n",
            "          0.0279, -0.0052,  0.0067,  0.0066,  0.0367, -0.0093,  0.0502, -0.0592,\n",
            "          0.0212,  0.0324,  0.0156, -0.0536,  0.0560, -0.0231, -0.0043,  0.0070,\n",
            "          0.0113, -0.0335, -0.0537, -0.0324,  0.0463, -0.0129,  0.0215, -0.0427,\n",
            "         -0.0580, -0.0037,  0.0549,  0.0574,  0.0150, -0.0076,  0.0351,  0.0035,\n",
            "          0.0289, -0.0269,  0.0410,  0.0478, -0.0283, -0.0131,  0.0600,  0.0420,\n",
            "         -0.0009,  0.0004, -0.0218,  0.0070,  0.0313, -0.0410, -0.0223,  0.0345,\n",
            "         -0.0316, -0.0285,  0.0600,  0.0464,  0.0239,  0.0096, -0.0557, -0.0258,\n",
            "         -0.0456,  0.0406,  0.0047,  0.0589,  0.0315, -0.0176, -0.0452,  0.0584,\n",
            "          0.0190,  0.0400,  0.0519, -0.0045, -0.0080, -0.0600, -0.0335,  0.0332,\n",
            "          0.0170, -0.0046, -0.0260,  0.0308,  0.0076, -0.0352,  0.0396,  0.0563,\n",
            "         -0.0413,  0.0181, -0.0277, -0.0261,  0.0330, -0.0062,  0.0348, -0.0435,\n",
            "         -0.0133,  0.0352, -0.0468, -0.0606,  0.0599,  0.0568,  0.0560, -0.0232,\n",
            "          0.0409, -0.0341, -0.0192, -0.0356,  0.0585,  0.0467, -0.0301,  0.0594,\n",
            "          0.0280,  0.0278,  0.0255, -0.0416,  0.0038,  0.0318, -0.0153, -0.0492,\n",
            "         -0.0574, -0.0418,  0.0340,  0.0165,  0.0240,  0.0448, -0.0528,  0.0100,\n",
            "         -0.0390,  0.0275,  0.0227,  0.0583, -0.0233, -0.0489,  0.0561, -0.0558,\n",
            "         -0.0553, -0.0078, -0.0423, -0.0386,  0.0428, -0.0333,  0.0094,  0.0376,\n",
            "          0.0530,  0.0142, -0.0226,  0.0533,  0.0025,  0.0274,  0.0595, -0.0055,\n",
            "          0.0580,  0.0380,  0.0353, -0.0147, -0.0506, -0.0446,  0.0067,  0.0175,\n",
            "         -0.0475, -0.0606,  0.0501, -0.0405,  0.0461, -0.0229,  0.0338,  0.0221,\n",
            "         -0.0240, -0.0155,  0.0561, -0.0058, -0.0074,  0.0579, -0.0273,  0.0196,\n",
            "          0.0434,  0.0545, -0.0534, -0.0101,  0.0297,  0.0057, -0.0086,  0.0025,\n",
            "          0.0618,  0.0396,  0.0206,  0.0027,  0.0362,  0.0472,  0.0520,  0.0501,\n",
            "          0.0255, -0.0153, -0.0575,  0.0139,  0.0622, -0.0447, -0.0412,  0.0553,\n",
            "         -0.0594, -0.0271,  0.0164,  0.0062, -0.0302,  0.0466,  0.0497, -0.0430,\n",
            "          0.0406,  0.0522,  0.0406,  0.0417,  0.0396, -0.0314,  0.0245, -0.0440,\n",
            "          0.0277,  0.0452, -0.0251,  0.0288,  0.0552,  0.0566, -0.0169, -0.0372,\n",
            "         -0.0302, -0.0269, -0.0392, -0.0488, -0.0281,  0.0209,  0.0163, -0.0194,\n",
            "         -0.0113, -0.0524,  0.0253,  0.0463,  0.0059,  0.0457,  0.0407,  0.0526]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 2.bias | Size: torch.Size([64]) | Values : tensor([-0.0079, -0.0098], grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 4.weight | Size: torch.Size([1, 64]) | Values : tensor([[ 0.0487, -0.0795, -0.0599, -0.0896,  0.0080, -0.0224, -0.0390, -0.0350,\n",
            "         -0.0664, -0.0256, -0.0995, -0.0461, -0.1072,  0.0499,  0.0461,  0.0642,\n",
            "          0.0575, -0.0985,  0.1139, -0.0822,  0.0719, -0.0653, -0.0382, -0.0622,\n",
            "          0.1102,  0.0887,  0.1002,  0.0071, -0.0427,  0.0934,  0.0856,  0.0015,\n",
            "         -0.1009, -0.0857, -0.0312, -0.0006,  0.0661,  0.0417, -0.1237, -0.0649,\n",
            "         -0.0117,  0.0695, -0.1217,  0.0672, -0.0451, -0.0078,  0.0445,  0.0435,\n",
            "         -0.0985,  0.1089, -0.0969, -0.0151,  0.0027, -0.0403,  0.0150,  0.0507,\n",
            "         -0.0312, -0.0945,  0.0606, -0.1250, -0.0743,  0.0043,  0.1004,  0.0823]],\n",
            "       grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: 4.bias | Size: torch.Size([1]) | Values : tensor([-0.1008], grad_fn=<SliceBackward>) \n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "X2OsLDncObD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "print('Using {} device'.format(device))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "metadata": {
        "id": "9HncBdmLObD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "class NeuralNetwork(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(NeuralNetwork, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(23,200)\r\n",
        "        self.fc2 = nn.Linear(200,300)\r\n",
        "        self.fc3 = nn.Linear(300,1)\r\n",
        "\r\n",
        "        # self.flatten = nn.Flatten()\r\n",
        "        # self.linear_relu_stack = nn.Sequential(\r\n",
        "        #     nn.Linear(23, 200),\r\n",
        "        #     nn.ReLU(),\r\n",
        "        #     nn.Linear(200, 1),\r\n",
        "        #     # nn.ReLU(),\r\n",
        "        #     # nn.Linear(250, 1),\r\n",
        "        #     nn.ReLU()\r\n",
        "        # )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        self.x1 = F.relu(self.fc1(x))\r\n",
        "        self.x2 = F.relu(self.fc2(self.x1))\r\n",
        "        x3 = F.sigmoid(self.fc3(self.x2))\r\n",
        "        return x3\r\n",
        "\r\n",
        "    "
      ],
      "outputs": [],
      "metadata": {
        "id": "g7fA1_L7ObD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "source": [
        "model = NeuralNetwork().to(device)\r\n",
        "param = list(model.parameters())\r\n",
        "print(len(list(model.parameters())))\r\n",
        "print(param[0].size())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "torch.Size([200, 23])\n"
          ]
        }
      ],
      "metadata": {
        "id": "3ElSF_TVObD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "model"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (fc1): Linear(in_features=23, out_features=200, bias=True)\n",
              "  (fc2): Linear(in_features=200, out_features=300, bias=True)\n",
              "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "metadata": {
        "id": "-WtebTokObD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "Bd8SswENObD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "source": [
        "logits = model(X_train.float())\r\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\r\n",
        "y_pred = pred_probab.argmax(1)\r\n",
        "# y_pred = y_pred.view(1,-1)\r\n",
        "# print(loss)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mat1 in method wrapper_addmm)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-47-bd849e13011c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpred_probab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_probab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# y_pred = y_pred.view(1,-1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-44-d2c2c62da7d8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mat1 in method wrapper_addmm)"
          ]
        }
      ],
      "metadata": {
        "id": "sSmIYGxNObD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "criterion = nn.MSELoss()\r\n",
        "\r\n",
        "loss = criterion(y_pred,y_train.float())\r\n",
        "loss = loss.float()\r\n",
        "loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "MdQRGnWjObD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.zero_grad()     # zeroes the gradient buffers of all parameters\r\n",
        "\r\n",
        "print('fc1.bias.grad before backward')\r\n",
        "print(model.fc1.bias.grad)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "kMaR57LoObD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "# create your optimizer\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\r\n",
        "\r\n",
        "# in your training loop:\r\n",
        "optimizer.zero_grad()   # zero the gradient buffers\r\n",
        "output = model(X_train.float())\r\n",
        "loss = criterion(output.float(), y_train.float())\r\n",
        "loss.backward()\r\n",
        "optimizer.step()    # Does the update"
      ],
      "outputs": [],
      "metadata": {
        "id": "ckNfKFmGObD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(X_train.size())\r\n",
        "print(X_test.size())\r\n",
        "print(y_train.size())\r\n",
        "print(y_test.size())"
      ],
      "outputs": [],
      "metadata": {
        "id": "3uaLwWJkObD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "layer1 = nn.Linear(in_features=23, out_features=200,device=device)\r\n",
        "hidden1 = layer1(X_train.float())\r\n",
        "print(hidden1.size())"
      ],
      "outputs": [],
      "metadata": {
        "id": "-lyFkhBbObD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\r\n",
        "hidden1 = nn.ReLU()(hidden1)\r\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "9H3cwjm4ObD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "seq_modules = nn.Sequential(\r\n",
        "    layer1,\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear\r\n",
        "    (\r\n",
        "        200,512,\r\n",
        "    device=device\r\n",
        "    ),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear\r\n",
        "    (\r\n",
        "        512, 256,\r\n",
        "    device =device\r\n",
        "    ),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear\r\n",
        "    (\r\n",
        "        256,1,device=device\r\n",
        "    ),\r\n",
        "    nn.Softmax(dim=1))\r\n",
        "# input_image = torch.rand(3,28,28)\r\n",
        "logits = seq_modules(X_train.float())"
      ],
      "outputs": [],
      "metadata": {
        "id": "XkYUDBWNObD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "y_train = torch.reshape(y_train, (24000, 1))\r\n",
        "y_train.size()"
      ],
      "outputs": [],
      "metadata": {
        "id": "-eX-iFYDObD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "softmax = nn.Softmax(dim=1)\r\n",
        "pred_probab = logits\r\n",
        "pred_probab.size()\r\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_probab, y_train.float())\r\n",
        "loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "WeOWpiMvObD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_train"
      ],
      "outputs": [],
      "metadata": {
        "id": "25T9sizFObD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pred_probab"
      ],
      "outputs": [],
      "metadata": {
        "id": "swlln-_HObD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"Model structure: \", seq_modules, \"\\n\\n\")\r\n",
        "\r\n",
        "for name, param in seq_modules.named_parameters():\r\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "sS4W8GSTObD_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "inp = torch.eye(5, requires_grad=True)\r\n",
        "out = (inp+1).pow(2)\r\n",
        "out.backward(torch.ones_like(inp), retain_graph=True)\r\n",
        "print(\"First call\\n\", inp.grad)\r\n",
        "out.backward(torch.ones_like(inp), retain_graph=True)\r\n",
        "print(\"\\nSecond call\\n\", inp.grad)\r\n",
        "inp.grad.zero_()\r\n",
        "out.backward(torch.ones_like(inp), retain_graph=True)\r\n",
        "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7mFf3EAiObD_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "G-susFjoObD_"
      }
    }
  ]
}