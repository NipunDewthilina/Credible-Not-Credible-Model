{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Input libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "source": [
    "import keras\r\n",
    "import numpy\r\n",
    "import pandas\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "source": [
    "import torch\r\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data input"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "source": [
    "# Importing the dataset\r\n",
    "dataset = pd.read_csv('default of credit card clients.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "source": [
    "import pandas as pd\r\n",
    "from sklearn import preprocessing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# X and Y distribution of dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "source": [
    "X = dataset.iloc[:, 1:24].values\r\n",
    "y = dataset.iloc[:, 24].values\r\n",
    "X.shape,y.shape\r\n",
    "type(y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 673
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "source": [
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30000, 23)"
      ]
     },
     "metadata": {},
     "execution_count": 674
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "source": [
    "import pandas as pd\r\n",
    "from sklearn import preprocessing\r\n",
    "\r\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\r\n",
    "X = min_max_scaler.fit_transform(X)\r\n",
    "df = pd.DataFrame(X)\r\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             0    1         2         3         4    5    6    7    8    9   \\\n",
       "0      0.010101  1.0  0.333333  0.333333  0.051724  0.4  0.4  0.1  0.1  0.0   \n",
       "1      0.111111  1.0  0.333333  0.666667  0.086207  0.1  0.4  0.2  0.2  0.2   \n",
       "2      0.080808  1.0  0.333333  0.666667  0.224138  0.2  0.2  0.2  0.2  0.2   \n",
       "3      0.040404  1.0  0.333333  0.333333  0.275862  0.2  0.2  0.2  0.2  0.2   \n",
       "4      0.040404  0.0  0.333333  0.333333  0.620690  0.1  0.2  0.1  0.2  0.2   \n",
       "...         ...  ...       ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "29995  0.212121  0.0  0.500000  0.333333  0.310345  0.2  0.2  0.2  0.2  0.2   \n",
       "29996  0.141414  0.0  0.500000  0.666667  0.379310  0.1  0.1  0.1  0.1  0.2   \n",
       "29997  0.020202  0.0  0.333333  0.666667  0.275862  0.6  0.5  0.4  0.1  0.2   \n",
       "29998  0.070707  0.0  0.500000  0.333333  0.344828  0.3  0.1  0.2  0.2  0.2   \n",
       "29999  0.040404  0.0  0.333333  0.333333  0.431034  0.2  0.2  0.2  0.2  0.2   \n",
       "\n",
       "       ...        13        14        15        16        17        18  \\\n",
       "0      ...  0.086723  0.160138  0.080648  0.260979  0.000000  0.000409   \n",
       "1      ...  0.087817  0.163220  0.084074  0.263485  0.000000  0.000594   \n",
       "2      ...  0.093789  0.173637  0.095470  0.272928  0.001738  0.000891   \n",
       "3      ...  0.113407  0.186809  0.109363  0.283685  0.002290  0.001199   \n",
       "4      ...  0.106020  0.179863  0.099633  0.275681  0.002290  0.021779   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "29995  ...  0.200746  0.243036  0.111622  0.273259  0.009730  0.011875   \n",
       "29996  ...  0.088267  0.168596  0.085794  0.260979  0.002103  0.002094   \n",
       "29997  ...  0.087859  0.179805  0.101057  0.275854  0.000000  0.000000   \n",
       "29998  ...  0.128239  0.209850  0.092403  0.298591  0.098334  0.002024   \n",
       "29999  ...  0.113667  0.194553  0.112803  0.272746  0.002379  0.001069   \n",
       "\n",
       "             19        20        21        22  \n",
       "0      0.000000  0.000000  0.000000  0.000000  \n",
       "1      0.001116  0.001610  0.000000  0.003783  \n",
       "2      0.001116  0.001610  0.002345  0.009458  \n",
       "3      0.001339  0.001771  0.002506  0.001892  \n",
       "4      0.011160  0.014493  0.001615  0.001284  \n",
       "...         ...       ...       ...       ...  \n",
       "29995  0.005583  0.004907  0.011723  0.001892  \n",
       "29996  0.010042  0.000208  0.000000  0.000000  \n",
       "29997  0.024552  0.006763  0.004689  0.005864  \n",
       "29998  0.001315  0.003101  0.124174  0.003412  \n",
       "29999  0.001596  0.001610  0.002345  0.001892  \n",
       "\n",
       "[30000 rows x 23 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086723</td>\n",
       "      <td>0.160138</td>\n",
       "      <td>0.080648</td>\n",
       "      <td>0.260979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087817</td>\n",
       "      <td>0.163220</td>\n",
       "      <td>0.084074</td>\n",
       "      <td>0.263485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.080808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093789</td>\n",
       "      <td>0.173637</td>\n",
       "      <td>0.095470</td>\n",
       "      <td>0.272928</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.009458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040404</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113407</td>\n",
       "      <td>0.186809</td>\n",
       "      <td>0.109363</td>\n",
       "      <td>0.283685</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.001892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106020</td>\n",
       "      <td>0.179863</td>\n",
       "      <td>0.099633</td>\n",
       "      <td>0.275681</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200746</td>\n",
       "      <td>0.243036</td>\n",
       "      <td>0.111622</td>\n",
       "      <td>0.273259</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.011875</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.011723</td>\n",
       "      <td>0.001892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088267</td>\n",
       "      <td>0.168596</td>\n",
       "      <td>0.085794</td>\n",
       "      <td>0.260979</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087859</td>\n",
       "      <td>0.179805</td>\n",
       "      <td>0.101057</td>\n",
       "      <td>0.275854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024552</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.005864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128239</td>\n",
       "      <td>0.209850</td>\n",
       "      <td>0.092403</td>\n",
       "      <td>0.298591</td>\n",
       "      <td>0.098334</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>0.124174</td>\n",
       "      <td>0.003412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113667</td>\n",
       "      <td>0.194553</td>\n",
       "      <td>0.112803</td>\n",
       "      <td>0.272746</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.001892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 23 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 675
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "source": [
    "X[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.11111111e-01, 1.00000000e+00, 3.33333333e-01, 6.66666667e-01,\n",
       "       8.62068966e-02, 1.00000000e-01, 4.00000000e-01, 2.00000000e-01,\n",
       "       2.00000000e-01, 2.00000000e-01, 4.00000000e-01, 1.48892434e-01,\n",
       "       6.78575089e-02, 8.78171337e-02, 1.63219937e-01, 8.40739510e-02,\n",
       "       2.63484742e-01, 0.00000000e+00, 5.93732912e-04, 1.11602161e-03,\n",
       "       1.61030596e-03, 0.00000000e+00, 3.78310691e-03])"
      ]
     },
     "metadata": {},
     "execution_count": 676
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "source": [
    "X[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.11111111e-01, 1.00000000e+00, 3.33333333e-01, 6.66666667e-01,\n",
       "       8.62068966e-02, 1.00000000e-01, 4.00000000e-01, 2.00000000e-01,\n",
       "       2.00000000e-01, 2.00000000e-01, 4.00000000e-01, 1.48892434e-01,\n",
       "       6.78575089e-02, 8.78171337e-02, 1.63219937e-01, 8.40739510e-02,\n",
       "       2.63484742e-01, 0.00000000e+00, 5.93732912e-04, 1.11602161e-03,\n",
       "       1.61030596e-03, 0.00000000e+00, 3.78310691e-03])"
      ]
     },
     "metadata": {},
     "execution_count": 677
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert Categorical to Numerical Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "source": [
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\r\n",
    "# #for country column\r\n",
    "# labelencoder_X_1 = LabelEncoder()\r\n",
    "# X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\r\n",
    "# #for gender column\r\n",
    "# labelencoder_X_2 = LabelEncoder()\r\n",
    "# X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\r\n",
    "# np.shape(X)\r\n",
    "# X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Baseline Model-Using Buildin Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "source": [
    "# # baseline model(using 1 connected layer and one 1 output layer)\r\n",
    "# def create_baseline():\r\n",
    "# # create model\r\n",
    "#     model = Sequential()\r\n",
    "#     model.add(Dense(30, input_dim=23, kernel_initializer='normal', activation='relu'))\r\n",
    "#     model.add(Dense(23, input_dim=30, kernel_initializer='normal', activation='relu'))\r\n",
    "#     model.add(Dense(30, input_dim=23, kernel_initializer='normal', activation='relu'))\r\n",
    "#     model.add(Dense(30, input_dim=23, kernel_initializer='normal', activation='relu'))\r\n",
    "#     model.add(Dense(1, kernel_initializer='random_normal', activation='tanh'))\r\n",
    "#     # Compile model\r\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "#     return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "source": [
    "# seed = 7\r\n",
    "# numpy.random.seed(seed)\r\n",
    "# # evaluate model with standardized dataset\r\n",
    "# estimator = KerasClassifier(build_fn=create_baseline, epochs=50, batch_size=5, verbose=0)\r\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\r\n",
    "# results = cross_val_score(estimator, X, y, cv=kfold)\r\n",
    "# print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Model-Using Sequential Module\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\r\n",
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2.02020202e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        1.77133655e-03, 3.41125691e-03, 1.46595393e-03],\n",
       "       [1.31313131e-01, 1.00000000e+00, 1.66666667e-01, ...,\n",
       "        8.05152979e-03, 1.40670388e-02, 9.45776729e-03],\n",
       "       [1.91919192e-01, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        4.83735910e-03, 7.04758645e-03, 7.58891247e-03],\n",
       "       ...,\n",
       "       [1.91919192e-01, 0.00000000e+00, 3.33333333e-01, ...,\n",
       "        8.05152979e-04, 2.34450647e-03, 2.07287777e-01],\n",
       "       [1.21212121e-01, 0.00000000e+00, 3.33333333e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [8.08080808e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        1.77133655e-03, 5.15791423e-03, 2.83733019e-03]])"
      ]
     },
     "metadata": {},
     "execution_count": 681
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "source": [
    "type(X_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 682
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "source": [
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2.02020202e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        1.77133655e-03, 3.41125691e-03, 1.46595393e-03],\n",
       "       [1.31313131e-01, 1.00000000e+00, 1.66666667e-01, ...,\n",
       "        8.05152979e-03, 1.40670388e-02, 9.45776729e-03],\n",
       "       [1.91919192e-01, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        4.83735910e-03, 7.04758645e-03, 7.58891247e-03],\n",
       "       ...,\n",
       "       [1.91919192e-01, 0.00000000e+00, 3.33333333e-01, ...,\n",
       "        8.05152979e-04, 2.34450647e-03, 2.07287777e-01],\n",
       "       [1.21212121e-01, 0.00000000e+00, 3.33333333e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [8.08080808e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        1.77133655e-03, 5.15791423e-03, 2.83733019e-03]])"
      ]
     },
     "metadata": {},
     "execution_count": 683
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "source": [
    "y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "metadata": {},
     "execution_count": 684
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "source": [
    "import tensorflow as tf\r\n",
    "# import keras\r\n",
    "# from keras import layers\r\n",
    "# from keras import models\r\n",
    "# from keras import utils\r\n",
    "# from keras.layers import Dense\r\n",
    "# from keras.models import Sequential\r\n",
    "# from keras.layers import Flatten\r\n",
    "# from keras.layers import Dropout\r\n",
    "# from keras.layers import Activation\r\n",
    "# from keras.regularizers import l2\r\n",
    "# from keras.optimizers import SGD\r\n",
    "# from keras.optimizers import RMSprop\r\n",
    "# from keras import datasets\r\n",
    "\r\n",
    "# from keras.callbacks import LearningRateScheduler\r\n",
    "# from keras.callbacks import History\r\n",
    "\r\n",
    "# from keras import losses\r\n",
    "# from sklearn.utils import shuffle\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "from tensorflow.keras.optimizers import SGD\r\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
    "print(tf.__version__)\r\n",
    "print(tf.keras.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.5.0\n",
      "2.5.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\r\n",
    "    initial_learning_rate=1e-2,\r\n",
    "    decay_steps=10000,\r\n",
    "    decay_rate=0.1)\r\n",
    "optimizer1 = keras.optimizers.SGD(learning_rate=lr_schedule)\r\n",
    "optimizer2 = keras.optimizers.Adam(learning_rate = 0.001,beta_1=0.9,\r\n",
    "    beta_2=0.999,\r\n",
    "    epsilon=1e-07)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "source": [
    "my_callbacks = [\r\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2),\r\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{loss:.2f}.h5'),\r\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "source": [
    "# #Initializing Neural Network\r\n",
    "# Model = keras.Sequential()\r\n",
    "# Model.add(Dense(100, input_dim = 23, kernel_initializer = 'random_uniform', activation = 'relu'))\r\n",
    "# # Adding the second hidden layer\r\n",
    "# Model.add(Dense(200, kernel_initializer = 'random_uniform', activation = 'relu'))\r\n",
    "# # Adding the third hidden layer\r\n",
    "# Model.add(Dense(300, kernel_initializer = 'uniform', activation = 'relu'))\r\n",
    "# # Adding the Fourth hidden layer\r\n",
    "# Model.add(Dense(200, kernel_initializer = 'random_uniform', activation = 'relu'))\r\n",
    "# # Adding the Fifth layer\r\n",
    "# Model.add(Dense(200, kernel_initializer = 'random_uniform', activation = 'relu'))\r\n",
    "# # Adding the output layer\r\n",
    "# Model.add(Dense(1, kernel_initializer = 'random_uniform', activation = 'sigmoid'))\r\n",
    "# # Compiling Neural Network\r\n",
    "# Model.compile(optimizer = optimizer2, loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n",
    "# #fitting the neural Network\r\n",
    "# early_stopping_monitor = EarlyStopping(patience=2)\r\n",
    "# Model.fit(X_train, y_train, batch_size = 15, epochs = 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "source": [
    "score = Model.evaluate(X_test, y_test, verbose=1)\r\n",
    "print('Accuracy: ', score[1]*100)\r\n",
    "print( 'loss:', score[0]*100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "188/188 [==============================] - 1s 2ms/step - loss: 0.4280 - accuracy: 0.8213\n",
      "Accuracy:  82.13333487510681\n",
      "loss: 42.79545843601227\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "source": [
    "# print(optimizer2.get_weights())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch Implementation\n",
    "## Import necessary libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "source": [
    "import torch\r\n",
    "# import keras\r\n",
    "import numpy\r\n",
    "import pandas\r\n",
    "# from keras.models import Sequential\r\n",
    "# from keras.layers import Dense\r\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process Start"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "source": [
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2.02020202e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        1.77133655e-03, 3.41125691e-03, 1.46595393e-03],\n",
       "       [1.31313131e-01, 1.00000000e+00, 1.66666667e-01, ...,\n",
       "        8.05152979e-03, 1.40670388e-02, 9.45776729e-03],\n",
       "       [1.91919192e-01, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        4.83735910e-03, 7.04758645e-03, 7.58891247e-03],\n",
       "       ...,\n",
       "       [1.91919192e-01, 0.00000000e+00, 3.33333333e-01, ...,\n",
       "        8.05152979e-04, 2.34450647e-03, 2.07287777e-01],\n",
       "       [1.21212121e-01, 0.00000000e+00, 3.33333333e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [8.08080808e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        1.77133655e-03, 5.15791423e-03, 2.83733019e-03]])"
      ]
     },
     "metadata": {},
     "execution_count": 692
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "source": [
    "y_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 693
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "source": [
    "X_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        8.05152979e-04, 7.03351941e-04, 3.87768459e-04],\n",
       "       [9.09090909e-02, 1.00000000e+00, 1.66666667e-01, ...,\n",
       "        9.50080515e-03, 7.26797006e-03, 0.00000000e+00],\n",
       "       [2.02020202e-02, 1.00000000e+00, 3.33333333e-01, ...,\n",
       "        9.66183575e-04, 6.89753803e-03, 1.13493207e-03],\n",
       "       ...,\n",
       "       [7.07070707e-02, 0.00000000e+00, 1.66666667e-01, ...,\n",
       "        0.00000000e+00, 9.57730893e-03, 0.00000000e+00],\n",
       "       [7.07070707e-02, 0.00000000e+00, 3.33333333e-01, ...,\n",
       "        4.02576490e-03, 7.03351941e-03, 9.45776729e-03],\n",
       "       [2.02020202e-02, 0.00000000e+00, 1.66666667e-01, ...,\n",
       "        2.31884058e-04, 3.00096828e-04, 7.37705848e-05]])"
      ]
     },
     "metadata": {},
     "execution_count": 694
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "source": [
    "y_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 695
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "source": [
    "data = [[1, 2],[3, 4]]\r\n",
    "x_data = torch.tensor(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "source": [
    "x_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 697
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert to Tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "source": [
    "# X_train = torch.tensor(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(24000, 23)"
      ]
     },
     "metadata": {},
     "execution_count": 699
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "source": [
    "y_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 700
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "source": [
    "y_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 701
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "source": [
    "import os\r\n",
    "import torch\r\n",
    "from torch import nn\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import datasets, transforms\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Empty Cache"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "source": [
    "y_train_tens=y_train.astype(float)\r\n",
    "X_test_tens = X_test.astype(float)\r\n",
    "y_test_tens = y_test.astype(float)\r\n",
    "X_train_tens=X_train.astype(float)\r\n",
    "y_train.shape[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "metadata": {},
     "execution_count": 703
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "source": [
    "import torch\r\n",
    "\r\n",
    "# X=X_iris_tr; Y=Y_iris_tr; X_val=X_iris_val; Y_val=Y_iris_val\r\n",
    "# del X, Y, X_val, Y_val\r\n",
    "\r\n",
    "def two_layer_regression_autograd_train(X, Y, X_val, Y_val, lr, nite):\r\n",
    "\r\n",
    "    dtype = torch.float\r\n",
    "    device = torch.device(\"cpu\")\r\n",
    "    # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\r\n",
    "\r\n",
    "    # N is batch size; D_in is input dimension;\r\n",
    "    # H is hidden dimension; D_out is output dimension.\r\n",
    "    N, D_in, H, D_out = 100, X.shape[1], 200, Y.shape[0]\r\n",
    "\r\n",
    "    # Setting requires_grad=False indicates that we do not need to compute gradients\r\n",
    "    # with respect to these Tensors during the backward pass.\r\n",
    "    X = torch.from_numpy(X).float().to(device = device)\r\n",
    "    Y = torch.from_numpy(Y).float().to(device = device)\r\n",
    "    X_val = torch.from_numpy(X_val).float().to(device = device)\r\n",
    "    Y_val = torch.from_numpy(Y_val).float().to(device = device)\r\n",
    "\r\n",
    "    # Create random Tensors for weights.\r\n",
    "    # Setting requires_grad=True indicates that we want to compute gradients with\r\n",
    "    # respect to these Tensors during the backward pass.\r\n",
    "    W1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\r\n",
    "    W2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\r\n",
    "\r\n",
    "    losses_tr, losses_val = list(), list()\r\n",
    "\r\n",
    "    learning_rate = lr\r\n",
    "    for t in range(nite):\r\n",
    "        # Forward pass: compute predicted y using operations on Tensors; these\r\n",
    "        # are exactly the same operations we used to compute the forward pass using\r\n",
    "        # Tensors, but we do not need to keep references to intermediate values since\r\n",
    "        # we are not implementing the backward pass by hand.\r\n",
    "        y_pred = X.mm(W1).clamp(min=0).mm(W2)\r\n",
    "\r\n",
    "        # Compute and print loss using operations on Tensors.\r\n",
    "        # Now loss is a Tensor of shape (1,)\r\n",
    "        # loss.item() gets the scalar value held in the loss.\r\n",
    "        loss = (y_pred - Y).pow(2).sum()\r\n",
    "\r\n",
    "        # Use autograd to compute the backward pass. This call will compute the\r\n",
    "        # gradient of loss with respect to all Tensors with requires_grad=True.\r\n",
    "        # After this call w1.grad and w2.grad will be Tensors holding the gradient\r\n",
    "        # of the loss with respect to w1 and w2 respectively.\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        # Manually update weights using gradient descent. Wrap in torch.no_grad()\r\n",
    "        # because weights have requires_grad=True, but we don't need to track this\r\n",
    "        # in autograd.\r\n",
    "        # An alternative way is to operate on weight.data and weight.grad.data.\r\n",
    "        # Recall that tensor.data gives a tensor that shares the storage with\r\n",
    "        # tensor, but doesn't track history.\r\n",
    "        # You can also use torch.optim.SGD to achieve this.\r\n",
    "        with torch.no_grad():\r\n",
    "            W1 -= learning_rate * W1.grad\r\n",
    "            W2 -= learning_rate * W2.grad\r\n",
    "\r\n",
    "            # Manually zero the gradients after updating weights\r\n",
    "            W1.grad.zero_()\r\n",
    "            W2.grad.zero_()\r\n",
    "\r\n",
    "            y_pred = X_val.mm(W1).clamp(min=0).mm(W2)\r\n",
    "\r\n",
    "            # Compute and print loss using operations on Tensors.\r\n",
    "            # Now loss is a Tensor of shape (1,)\r\n",
    "            # loss.item() gets the scalar value held in the loss.\r\n",
    "            loss_val = (y_pred - Y).pow(2).sum()\r\n",
    "\r\n",
    "        if t % 10 == 0:\r\n",
    "            print(t, loss.item(), loss_val.item())\r\n",
    "\r\n",
    "        losses_tr.append(loss.item())\r\n",
    "        losses_val.append(loss_val.item())\r\n",
    "\r\n",
    "    return W1, W2, losses_tr, losses_val\r\n",
    "\r\n",
    "W1, W2, losses_tr, losses_val = two_layer_regression_autograd_train(X=X_train, Y=y_train, X_val=X_test, Y_val=y_test,\r\n",
    "                                                                 lr=1e-4, nite=50)\r\n",
    "plt.plot(np.arange(len(losses_tr)), losses_tr, \"-b\", np.arange(len(losses_val)), losses_val, \"-r\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:79] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2304000000 bytes.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-707-380148fe9e45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m W1, W2, losses_tr, losses_val = two_layer_regression_autograd_train(X=X_train, Y=y_train, X_val=X_test, Y_val=y_test,\n\u001b[0m\u001b[0;32m     81\u001b[0m                                                                  lr=1e-4, nite=50)\n\u001b[0;32m     82\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-707-380148fe9e45>\u001b[0m in \u001b[0;36mtwo_layer_regression_autograd_train\u001b[1;34m(X, Y, X_val, Y_val, lr, nite)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Now loss is a Tensor of shape (1,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# loss.item() gets the scalar value held in the loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# Use autograd to compute the backward pass. This call will compute the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:79] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2304000000 bytes."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "source": [
    "y_train =torch.tensor(y_train_tens,requires_grad = True).to(device=device)\r\n",
    "X_test  =torch.tensor(X_test_tens  ,requires_grad = True).to(device=device)\r\n",
    "y_test  =torch.tensor(y_test_tens  ,requires_grad = True).to(device=device)\r\n",
    "X_train =torch.tensor(X_train_tens,requires_grad = True).to(device=device)\r\n",
    "# y_train=y_train.transpose(-1,0)\r\n",
    "# X_test = X_test.transpose(-1,0 )\r\n",
    "# y_test = y_test.transpose(-1,0 )\r\n",
    "# X_train=X_train.transpose(-1,0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "print('Using {} device'.format(device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "source": [
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "        self.fc1 = nn.Linear(23,200)\r\n",
    "        self.fc2 = nn.Linear(200,300)\r\n",
    "        self.fc3 = nn.Linear(300,1)\r\n",
    "\r\n",
    "        # self.flatten = nn.Flatten()\r\n",
    "        # self.linear_relu_stack = nn.Sequential(\r\n",
    "        #     nn.Linear(23, 200),\r\n",
    "        #     nn.ReLU(),\r\n",
    "        #     nn.Linear(200, 1),\r\n",
    "        #     # nn.ReLU(),\r\n",
    "        #     # nn.Linear(250, 1),\r\n",
    "        #     nn.ReLU()\r\n",
    "        # )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        self.x1 = F.relu(self.fc1(x))\r\n",
    "        self.x2 = F.relu(self.fc2(self.x1))\r\n",
    "        x3 = F.sigmoid(self.fc3(self.x2))\r\n",
    "        return x3\r\n",
    "\r\n",
    "    def backward(self,loss,x3):\r\n",
    "        d1 = F.mse_loss(loss,x3)\r\n",
    "        \r\n",
    "        \r\n",
    "    \r\n",
    "    def train(self, X, l):\r\n",
    "        # Forward propagation\r\n",
    "        x3 = self.forward(X)\r\n",
    "\r\n",
    "        # Backward propagation and gradient descent\r\n",
    "        self.backward(X, l, x3)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "source": [
    "model = NeuralNetwork().to(device)\r\n",
    "param = list(model.parameters())\r\n",
    "print(len(list(model.parameters())))\r\n",
    "print(param[0].size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n",
      "torch.Size([200, 23])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=23, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=300, bias=True)\n",
       "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 549
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "source": [
    "logits = model(X_train.float())\r\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\r\n",
    "y_pred = pred_probab.argmax(1)\r\n",
    "# y_pred = y_pred.view(1,-1)\r\n",
    "# print(loss)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.20 GiB already allocated; 0 bytes free; 1.30 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-550-bd849e13011c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpred_probab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_probab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# y_pred = y_pred.view(1,-1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-547-964414ffcfcc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.20 GiB already allocated; 0 bytes free; 1.30 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "source": [
    "criterion = nn.MSELoss()\r\n",
    "\r\n",
    "loss = criterion(y_pred,y_train.float())\r\n",
    "loss = loss.float()\r\n",
    "loss"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.2215, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 535
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "source": [
    "model.zero_grad()     # zeroes the gradient buffers of all parameters\r\n",
    "\r\n",
    "print('fc1.bias.grad before backward')\r\n",
    "print(model.fc1.bias.grad)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fc1.bias.grad before backward\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "source": [
    "import torch.optim as optim\r\n",
    "\r\n",
    "# create your optimizer\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\r\n",
    "\r\n",
    "# in your training loop:\r\n",
    "optimizer.zero_grad()   # zero the gradient buffers\r\n",
    "output = model(X_train.float())\r\n",
    "loss = criterion(output.float(), y_train.float())\r\n",
    "loss.backward()\r\n",
    "optimizer.step()    # Does the update"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 2.00 GiB total capacity; 1.20 GiB already allocated; 0 bytes free; 1.30 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-545-88ca15354240>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# in your training loop:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# zero the gradient buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-515-964414ffcfcc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 2.00 GiB total capacity; 1.20 GiB already allocated; 0 bytes free; 1.30 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "source": [
    "print(X_train.size())\r\n",
    "print(X_test.size())\r\n",
    "print(y_train.size())\r\n",
    "print(y_test.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([24000, 23])\n",
      "torch.Size([6000, 23])\n",
      "torch.Size([24000, 1])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "source": [
    "layer1 = nn.Linear(in_features=23, out_features=200,device=device)\r\n",
    "hidden1 = layer1(X_train.float())\r\n",
    "print(hidden1.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([24000, 200])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\r\n",
    "hidden1 = nn.ReLU()(hidden1)\r\n",
    "print(f\"After ReLU: {hidden1}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before ReLU: tensor([[ 0.2243, -0.2964, -0.1045,  ...,  0.0027, -0.4305, -0.1404],\n",
      "        [ 0.2262, -0.3016, -0.1059,  ..., -0.0023, -0.4509, -0.1095],\n",
      "        [ 0.1858, -0.3416, -0.2001,  ..., -0.0284, -0.3508, -0.1391],\n",
      "        ...,\n",
      "        [ 0.1000, -0.1960,  0.0037,  ...,  0.1670, -0.3378,  0.1439],\n",
      "        [ 0.0984, -0.1800,  0.0675,  ...,  0.0438, -0.2134,  0.0036],\n",
      "        [ 0.1444, -0.1871,  0.0893,  ...,  0.0098, -0.2346, -0.0534]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.2243, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],\n",
      "        [0.2262, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1858, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1000, 0.0000, 0.0037,  ..., 0.1670, 0.0000, 0.1439],\n",
      "        [0.0984, 0.0000, 0.0675,  ..., 0.0438, 0.0000, 0.0036],\n",
      "        [0.1444, 0.0000, 0.0893,  ..., 0.0098, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "source": [
    "seq_modules = nn.Sequential(\r\n",
    "    layer1,\r\n",
    "    nn.ReLU(),\r\n",
    "    nn.Linear\r\n",
    "    (\r\n",
    "        200,512,\r\n",
    "    device=device\r\n",
    "    ),\r\n",
    "    nn.ReLU(),\r\n",
    "    nn.Linear\r\n",
    "    (\r\n",
    "        512, 256,\r\n",
    "    device =device\r\n",
    "    ),\r\n",
    "    nn.ReLU(),\r\n",
    "    nn.Linear\r\n",
    "    (\r\n",
    "        256,1,device=device\r\n",
    "    ),\r\n",
    "    nn.Softmax(dim=1))\r\n",
    "# input_image = torch.rand(3,28,28)\r\n",
    "logits = seq_modules(X_train.float())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "source": [
    "\r\n",
    "y_train = torch.reshape(y_train, (24000, 1))\r\n",
    "y_train.size()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([24000, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 246
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "source": [
    "softmax = nn.Softmax(dim=1)\r\n",
    "pred_probab = logits\r\n",
    "pred_probab.size()\r\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_probab, y_train.float())\r\n",
    "loss"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.0911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 247
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "source": [
    "y_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]], device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 248
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "source": [
    "pred_probab"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 249
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "source": [
    "print(\"Model structure: \", seq_modules, \"\\n\\n\")\r\n",
    "\r\n",
    "for name, param in seq_modules.named_parameters():\r\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model structure:  Sequential(\n",
      "  (0): Linear(in_features=23, out_features=200, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=200, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ") \n",
      "\n",
      "\n",
      "Layer: 0.weight | Size: torch.Size([200, 23]) | Values : tensor([[ 0.0694, -0.1479, -0.1050, -0.0477,  0.1768, -0.1640, -0.2053, -0.2023,\n",
      "          0.1288, -0.0069,  0.1991,  0.0728, -0.1491,  0.1413, -0.0040,  0.0661,\n",
      "          0.1873, -0.1845,  0.0394, -0.1164, -0.0935,  0.1874,  0.0176],\n",
      "        [ 0.2016, -0.0173, -0.1691,  0.0954,  0.0678, -0.1847,  0.1201,  0.2047,\n",
      "          0.0838,  0.0665, -0.1913,  0.1045,  0.1565,  0.0428,  0.1819,  0.0788,\n",
      "          0.1851,  0.1662, -0.1455,  0.0979, -0.1358,  0.1750,  0.0926]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 0.bias | Size: torch.Size([200]) | Values : tensor([0.1192, 0.2022], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 2.weight | Size: torch.Size([512, 200]) | Values : tensor([[-3.2550e-03, -2.3073e-02,  6.8856e-02,  4.5061e-02, -1.2318e-02,\n",
      "         -2.5888e-02, -5.6405e-02, -1.9036e-02,  4.1662e-02, -5.4406e-02,\n",
      "          6.5509e-02,  3.6587e-02, -6.3486e-02,  2.0041e-02,  3.9571e-02,\n",
      "          5.8120e-02, -1.4970e-02, -5.1826e-02,  4.0175e-02, -9.6879e-03,\n",
      "         -1.7374e-02, -6.0268e-02, -2.3113e-02,  3.8898e-02,  5.1029e-02,\n",
      "          5.2845e-02,  1.7976e-02, -4.2876e-02,  1.0439e-02, -5.2472e-02,\n",
      "         -1.3564e-03, -5.3359e-02, -6.4908e-03,  6.6305e-03,  6.3324e-02,\n",
      "         -4.1470e-02, -1.4771e-02,  4.8295e-02,  9.1089e-03,  1.3900e-02,\n",
      "         -6.3307e-02, -4.9735e-02, -4.2063e-02,  4.3384e-02,  5.8180e-02,\n",
      "          6.6283e-03,  5.9136e-02,  5.0081e-02, -2.0692e-02, -2.4396e-02,\n",
      "          3.5037e-03, -4.0724e-02,  6.1914e-03,  5.7095e-02,  2.3587e-03,\n",
      "          4.0700e-02, -5.6206e-02,  4.7526e-02,  2.0817e-02,  5.7300e-02,\n",
      "         -2.6463e-02,  3.2848e-02,  6.8840e-02,  6.8795e-02,  6.4760e-02,\n",
      "         -7.9462e-03,  2.8960e-02,  1.9812e-02,  5.2738e-02, -2.5578e-02,\n",
      "         -3.3083e-02, -3.5188e-02, -3.8707e-02,  4.2247e-02,  1.8959e-02,\n",
      "         -3.7120e-02,  6.0492e-03, -9.0072e-03, -4.8216e-02,  4.4352e-02,\n",
      "          3.0033e-02, -3.0611e-02,  6.5029e-02,  1.7489e-02,  2.7545e-02,\n",
      "         -1.7121e-02,  1.3831e-03, -5.1371e-02,  5.6502e-02, -2.7208e-02,\n",
      "          6.0783e-02,  4.7680e-02, -3.7806e-02,  4.1950e-02,  1.9711e-02,\n",
      "          5.8965e-02, -2.0601e-02,  2.9478e-02, -6.8005e-02,  1.8599e-02,\n",
      "          4.5096e-02,  5.5691e-02,  2.9417e-03,  3.4754e-02,  6.7499e-02,\n",
      "         -5.3526e-02, -6.7046e-02,  5.5491e-02,  3.9819e-02, -4.1294e-02,\n",
      "          6.6136e-02, -6.1684e-02,  4.8017e-02, -5.2608e-02, -2.4247e-02,\n",
      "          3.2229e-02,  1.7105e-02,  3.5459e-02, -4.3157e-02,  6.0851e-02,\n",
      "         -6.7773e-02, -4.8941e-02, -5.0695e-02,  1.8122e-02,  1.9320e-05,\n",
      "         -5.0999e-02,  5.3300e-02, -2.7462e-02, -2.9364e-03, -3.4816e-02,\n",
      "         -3.4214e-02,  1.8803e-02,  4.3279e-02,  2.0934e-02, -6.9833e-02,\n",
      "         -6.2709e-02,  5.1337e-02,  6.5710e-02, -5.9454e-02,  6.4500e-02,\n",
      "          5.7639e-02, -3.3725e-02, -4.6222e-02, -4.1272e-03, -4.4661e-02,\n",
      "         -3.3460e-02,  3.9628e-02, -4.1206e-02, -4.7904e-02,  5.1481e-02,\n",
      "          2.9902e-02,  1.5666e-02,  2.1948e-02, -4.2686e-03,  6.3269e-02,\n",
      "          4.2917e-02,  6.0349e-02, -2.5160e-02, -5.3133e-02, -5.2999e-02,\n",
      "         -3.7622e-03,  1.0274e-02,  2.8444e-02, -2.2399e-02,  4.9288e-02,\n",
      "         -4.4029e-02, -3.4468e-02, -6.0424e-02,  2.2750e-02,  1.3461e-02,\n",
      "         -6.8044e-03,  5.8297e-02, -1.3918e-02,  1.6672e-02,  4.0799e-02,\n",
      "         -4.6604e-02, -3.5253e-04,  4.3543e-02,  6.7531e-02,  6.4614e-02,\n",
      "          4.2095e-02,  4.8011e-02,  5.3358e-02, -3.1017e-02,  3.5029e-02,\n",
      "          8.5542e-03, -4.0634e-02,  3.6998e-02,  1.2525e-02, -5.4337e-02,\n",
      "         -1.8231e-02, -5.5696e-03, -1.9112e-02, -3.8294e-02, -4.4912e-02,\n",
      "          1.7696e-02, -6.9058e-02,  4.7632e-02, -1.8050e-02, -4.3837e-02],\n",
      "        [-5.8346e-02, -1.4405e-02,  6.0684e-02,  4.2299e-05,  3.0353e-02,\n",
      "          3.1859e-02, -1.8258e-02, -1.2790e-02, -3.5052e-02, -4.1382e-02,\n",
      "         -3.7540e-03, -3.4579e-02, -4.3470e-02, -4.2477e-02, -9.6343e-03,\n",
      "          4.5064e-02, -2.2685e-02, -4.6710e-02, -1.3851e-02, -4.9817e-02,\n",
      "          4.9933e-02, -4.2496e-02,  3.8118e-02,  2.7910e-02, -1.3732e-02,\n",
      "          5.3854e-02,  9.9968e-03,  3.1190e-02,  7.0484e-02,  1.7339e-02,\n",
      "          2.9477e-02,  4.0485e-02,  7.4283e-03, -6.3770e-02,  4.9468e-02,\n",
      "         -5.4382e-02, -3.5240e-02, -5.7252e-02, -2.9853e-02, -7.3200e-04,\n",
      "         -1.8620e-02,  6.6036e-02, -2.6483e-03,  3.8238e-02, -2.1687e-02,\n",
      "         -4.8281e-02, -5.4824e-02,  1.1884e-02, -5.0350e-02, -5.0010e-02,\n",
      "         -1.7636e-03, -4.8157e-02,  4.9213e-02,  4.0190e-02,  1.0556e-03,\n",
      "          4.1328e-02,  1.3651e-02,  2.8216e-02, -1.2930e-02,  5.7935e-02,\n",
      "         -3.4513e-02,  1.5865e-02, -6.6917e-02,  2.7812e-02,  2.0690e-02,\n",
      "          3.4419e-02, -3.4128e-02, -2.2089e-03,  1.3217e-02,  4.7020e-02,\n",
      "         -6.9762e-02, -6.9132e-02,  1.6989e-02, -6.4768e-02, -6.5465e-02,\n",
      "          4.4549e-02, -5.4261e-02, -5.9657e-02, -6.9068e-02, -5.0579e-02,\n",
      "          5.7136e-02, -4.1308e-02, -5.9902e-02,  2.9872e-02, -4.7431e-02,\n",
      "          2.1389e-02,  4.2253e-03,  6.6815e-02, -2.8949e-02,  1.6049e-02,\n",
      "         -8.2035e-05,  4.9560e-02,  3.2915e-02, -4.5989e-02, -4.1670e-02,\n",
      "          3.5708e-02,  3.3702e-02,  5.8308e-02,  2.8129e-02, -9.1956e-03,\n",
      "         -2.5163e-02,  5.1648e-02, -1.3768e-02,  1.7389e-02, -2.6245e-02,\n",
      "          1.3319e-02, -4.3484e-02,  5.0361e-02, -3.3604e-02,  4.2365e-02,\n",
      "         -1.9717e-02,  3.5715e-02, -5.3231e-02,  7.0085e-02, -6.8375e-02,\n",
      "         -8.1437e-03, -1.9812e-02, -2.3861e-02,  3.0907e-02,  1.5589e-02,\n",
      "          6.8442e-02,  4.5436e-02, -4.0559e-02, -5.5164e-02,  7.0563e-02,\n",
      "          2.9595e-02, -6.1297e-02, -1.2408e-03, -2.0693e-02, -5.4881e-02,\n",
      "          5.8158e-02, -3.1653e-02, -6.0366e-02, -2.1216e-02,  4.4032e-02,\n",
      "          2.3380e-02, -4.3532e-02, -4.2499e-02,  4.9441e-04,  5.6597e-02,\n",
      "          6.8069e-02, -1.1917e-02, -4.4989e-02, -5.1203e-02, -1.1582e-02,\n",
      "         -3.8442e-02,  6.9289e-02,  1.6922e-02,  1.3181e-02,  5.0315e-02,\n",
      "         -5.7748e-02,  5.4868e-02, -6.3729e-02, -6.1807e-02, -3.2927e-03,\n",
      "          4.6305e-03, -6.3434e-02,  6.2708e-02, -2.0351e-02,  1.5249e-02,\n",
      "         -7.0256e-02,  8.8610e-04,  4.6134e-02, -1.7395e-02,  2.9269e-02,\n",
      "         -5.4657e-03,  6.5557e-02,  6.7802e-02,  6.8362e-02, -2.7325e-03,\n",
      "         -5.8980e-02, -4.5498e-02, -5.6316e-02,  3.4732e-02, -4.8656e-02,\n",
      "         -5.4362e-02,  6.4546e-02, -2.7190e-02,  5.6307e-03,  4.0152e-02,\n",
      "          2.5592e-03,  6.6213e-02, -2.7143e-02, -1.1310e-03,  3.1586e-02,\n",
      "          2.0607e-02,  1.6038e-02, -2.4523e-03,  3.4686e-02, -2.4009e-02,\n",
      "         -5.1960e-02,  5.7519e-04,  5.6978e-03, -5.8892e-03, -5.0000e-02,\n",
      "         -2.6286e-02,  6.2325e-02,  6.8572e-02, -3.2749e-02,  8.2106e-03]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 2.bias | Size: torch.Size([512]) | Values : tensor([-0.0357, -0.0368], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 4.weight | Size: torch.Size([256, 512]) | Values : tensor([[ 0.0205,  0.0332,  0.0335,  ..., -0.0301, -0.0156,  0.0161],\n",
      "        [-0.0075, -0.0373, -0.0362,  ...,  0.0301,  0.0121, -0.0234]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 4.bias | Size: torch.Size([256]) | Values : tensor([-0.0381, -0.0107], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 6.weight | Size: torch.Size([1, 256]) | Values : tensor([[ 0.0384, -0.0394, -0.0333, -0.0406, -0.0382,  0.0154,  0.0418,  0.0479,\n",
      "          0.0218,  0.0286,  0.0442,  0.0016,  0.0510, -0.0590, -0.0195,  0.0155,\n",
      "         -0.0093,  0.0469, -0.0361,  0.0094, -0.0049,  0.0050,  0.0576, -0.0314,\n",
      "         -0.0485, -0.0548, -0.0365,  0.0033,  0.0245, -0.0335, -0.0296, -0.0589,\n",
      "          0.0301, -0.0296,  0.0525, -0.0398,  0.0071, -0.0242,  0.0498, -0.0601,\n",
      "          0.0366, -0.0447,  0.0196, -0.0087,  0.0092, -0.0063, -0.0512, -0.0383,\n",
      "         -0.0499,  0.0009, -0.0556, -0.0358, -0.0200,  0.0088, -0.0421, -0.0531,\n",
      "         -0.0414,  0.0530, -0.0523, -0.0570,  0.0064, -0.0527,  0.0139,  0.0291,\n",
      "         -0.0335, -0.0590, -0.0167,  0.0203,  0.0540,  0.0374, -0.0395,  0.0615,\n",
      "          0.0164, -0.0522, -0.0507,  0.0373, -0.0455, -0.0396,  0.0128,  0.0080,\n",
      "          0.0162,  0.0259,  0.0608,  0.0276, -0.0425,  0.0172,  0.0262, -0.0551,\n",
      "         -0.0022,  0.0048, -0.0348,  0.0251,  0.0427, -0.0109, -0.0143, -0.0153,\n",
      "         -0.0239, -0.0371, -0.0306, -0.0473,  0.0522, -0.0413, -0.0401, -0.0297,\n",
      "          0.0180,  0.0357, -0.0109, -0.0055, -0.0323,  0.0134, -0.0418,  0.0561,\n",
      "         -0.0182, -0.0501, -0.0161, -0.0326, -0.0113, -0.0441, -0.0563,  0.0116,\n",
      "         -0.0240, -0.0008,  0.0135,  0.0483,  0.0512, -0.0442,  0.0024,  0.0102,\n",
      "          0.0098,  0.0484, -0.0196, -0.0186,  0.0441,  0.0228, -0.0448,  0.0165,\n",
      "         -0.0030,  0.0066, -0.0589,  0.0090,  0.0414, -0.0555,  0.0116, -0.0572,\n",
      "          0.0285,  0.0457,  0.0587, -0.0607,  0.0029,  0.0495, -0.0438, -0.0555,\n",
      "          0.0562,  0.0483, -0.0293,  0.0046, -0.0470, -0.0081,  0.0169, -0.0264,\n",
      "         -0.0578,  0.0386,  0.0139,  0.0283,  0.0527, -0.0244, -0.0364, -0.0463,\n",
      "         -0.0268, -0.0494,  0.0167, -0.0392,  0.0573,  0.0086,  0.0037, -0.0508,\n",
      "         -0.0175, -0.0323, -0.0273,  0.0156,  0.0372, -0.0590,  0.0567, -0.0498,\n",
      "          0.0373, -0.0311,  0.0536,  0.0559,  0.0143,  0.0032,  0.0600, -0.0437,\n",
      "         -0.0240, -0.0203,  0.0228,  0.0142,  0.0537, -0.0310,  0.0550, -0.0429,\n",
      "         -0.0325, -0.0290,  0.0505,  0.0440,  0.0080,  0.0618,  0.0468, -0.0218,\n",
      "          0.0474,  0.0295,  0.0247,  0.0382,  0.0580, -0.0443,  0.0582, -0.0545,\n",
      "          0.0160, -0.0037, -0.0528,  0.0467, -0.0399, -0.0145, -0.0369,  0.0038,\n",
      "          0.0012,  0.0174, -0.0121,  0.0186,  0.0324,  0.0167,  0.0314,  0.0317,\n",
      "         -0.0425,  0.0513,  0.0572,  0.0106,  0.0033, -0.0171,  0.0065, -0.0604,\n",
      "          0.0135,  0.0130,  0.0346,  0.0331,  0.0409, -0.0050,  0.0533, -0.0405,\n",
      "          0.0298,  0.0571,  0.0079, -0.0517, -0.0198, -0.0138,  0.0534, -0.0449]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 6.bias | Size: torch.Size([1]) | Values : tensor([-0.0299], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "source": [
    "inp = torch.eye(5, requires_grad=True)\r\n",
    "out = (inp+1).pow(2)\r\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\r\n",
    "print(\"First call\\n\", inp.grad)\r\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\r\n",
    "print(\"\\nSecond call\\n\", inp.grad)\r\n",
    "inp.grad.zero_()\r\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\r\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}